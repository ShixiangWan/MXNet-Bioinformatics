{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Zillow Prediction\n",
    "\n",
    "* dtypes, fillnull, datatime\n",
    "* 相关性分析，去除不相关的特征\n",
    "* 确定问题是回归问题\n",
    "* 选择模型直接的相关度，进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "# from scipy.stats import uniform, randint\n",
    "\n",
    "# Model Persistence\n",
    "# import cPickle\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn import model_selection, preprocessing, metrics\n",
    "from sklearn import ensemble, linear_model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, train_test_split, StratifiedKFold\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, scorer, make_scorer\n",
    "\n",
    "# XGBoost\n",
    "# import xgboost as xgb\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "# Ensemble Learning\n",
    "from mlens.ensemble import SuperLearner, SequentialEnsemble\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "from mlens.preprocessing import EnsembleTransformer\n",
    "from mlens.visualization import corr_X_y, corrmat\n",
    "\n",
    "# Imbalanced Learning\n",
    "# import imblearn\n",
    "# from imblearn.combine import SMOTETomek\n",
    "\n",
    "# Count\n",
    "from collections import Counter\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Random Seed\n",
    "SEED = 2017\n",
    "\n",
    "\"\"\"\n",
    "载入原始数据集，保留所有的数据特点，\n",
    "用于特征工程分析、半监督学习\n",
    "\"\"\"\n",
    "def load_original():\n",
    "    # u'parcelid', ...... (total 58 columns)\n",
    "    properties16 = pd.read_csv('Data/properties_2016.csv')             # 特征数据集，无重复parcelid\n",
    "    # u'parcelid', u'logerror', u'transactiondate\n",
    "    train16 = pd.read_csv('Data/train_2016_v2.csv', parse_dates=[\"transactiondate\"])   # 训练数据集，无重复行,但有重复parcelid\n",
    "    # u'ParcelId', u'201610', u'201611', u'201612', u'201710', u'201711', u'201712'\n",
    "    sample_submission = pd.read_csv('Data/sample_submission.csv')      # 测试数据集，无重复ParcelId\n",
    "\n",
    "    print 'properties16.shape\\t', properties16.shape\n",
    "    print 'train16.shape\\t\\t', train16.shape\n",
    "    print 'sample_submission.shape\\t', sample_submission.shape\n",
    "    \n",
    "    return properties16, train16, sample_submission\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "载入训练数据集，放弃特征数据集中的无标签/logerror数据，\n",
    "用于有监督学习\n",
    "\"\"\"\n",
    "def load_supervision():\n",
    "    \"\"\"Read in training data and return input, output, columns tuple.\"\"\"\n",
    "\n",
    "    # This is a version of Anovas minimally prepared dataset\n",
    "    # for the xgbstarter script\n",
    "    # https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n",
    "\n",
    "    df = pd.read_csv('Data/train_2016_v2.csv')\n",
    "\n",
    "    prop = pd.read_csv('Data/properties_2016.csv')\n",
    "    convert = prop.dtypes == 'float64'\n",
    "    prop.loc[:, convert] = prop.loc[:, convert].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    df = df.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "    y = df.logerror\n",
    "    df = df.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "\n",
    "    convert = df.dtypes == 'object'\n",
    "    df.loc[:, convert] = df.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # test\n",
    "    sample = pd.read_csv('Data/sample_submission.csv')\n",
    "    sample['parcelid'] = sample['ParcelId']\n",
    "    sample = sample[['parcelid']].merge(prop, how='left', on='parcelid')\n",
    "    sample = sample.drop(['parcelid', 'propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "    \n",
    "    convert = sample.dtypes == 'object'\n",
    "    sample.loc[:, convert] = sample.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "    \n",
    "    sample.fillna(0, inplace=True)\n",
    "    \n",
    "    return df, y, sample, df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**分析特征交叉，产生新特征**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X, y, X_test, columns = load_supervision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape: (90275, 54)\n",
      "y.shape: (90275L,)\n",
      "X_test.shape: (2985217, 54)\n"
     ]
    }
   ],
   "source": [
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corr(X)\n",
      "[[ 1.    0.51 -0.1  -0.03]\n",
      " [ 0.51  1.    0.   -0.02]\n",
      " [-0.1   0.    1.    0.49]\n",
      " [-0.03 -0.02  0.49  1.  ]]\n",
      "Corr(Y)\n",
      "[[ 1.    0.51  0.06 -0.01]\n",
      " [ 0.51  1.   -0.01  0.02]\n",
      " [ 0.06 -0.01  1.    0.49]\n",
      " [-0.01  0.02  0.49  1.  ]]\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "# 2 latents vars:\n",
    "l1 = np.random.normal(size=n)\n",
    "l2 = np.random.normal(size=n)\n",
    "latents = np.array([l1, l1, l2, l2]).T\n",
    "X = latents + np.random.normal(size=4*n).reshape((n, 4))\n",
    "Y = latents + np.random.normal(size=4*n).reshape((n, 4))\n",
    "X_train = X[:n / 2]\n",
    "y_train = Y[:n / 2]\n",
    "X_valid = X[n / 2:]\n",
    "y_valid = Y[n / 2:]\n",
    "print(\"Corr(X)\")\n",
    "print(np.round(np.corrcoef(X.T), 2))\n",
    "print(\"Corr(Y)\")\n",
    "print(np.round(np.corrcoef(Y.T), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import PLSCanonical, PLSRegression, CCA\n",
    "\n",
    "plsca = PLSCanonical(n_components=2)\n",
    "plsca.fit(X_train, y_train)\n",
    "X_train_r, y_train_r = plsca.transform(X_train, y_train)\n",
    "X_valid_r, y_valid_r = plsca.transform(X_valid, y_valid)\n",
    "\n",
    "# print metrics.mean_absolute_error(y_train, y_train_r)\n",
    "# print metrics.mean_absolute_error(y_valid, y_valid_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.22122168, -0.68852125],\n",
       "       [ 1.31097076,  0.41774589],\n",
       "       [-1.31034155, -0.28116589],\n",
       "       [ 2.53023239, -1.50858901],\n",
       "       [-1.73196868, -3.8373544 ],\n",
       "       [ 2.08869478, -0.88579433],\n",
       "       [ 1.33257415, -0.44671041],\n",
       "       [-0.45580325, -0.73192746],\n",
       "       [ 2.93964553,  1.59389909],\n",
       "       [-0.98382292, -0.23253507],\n",
       "       [ 0.86426913, -1.39510186],\n",
       "       [-0.19432777, -0.12186885],\n",
       "       [ 1.20461515,  1.48448805],\n",
       "       [ 2.04159301,  1.51152167],\n",
       "       [ 0.77391181, -0.91013512],\n",
       "       [ 0.09224536, -0.6229514 ],\n",
       "       [-0.07123024,  0.87560973],\n",
       "       [ 1.18279988, -0.42656831],\n",
       "       [ 2.14299478, -1.36177214],\n",
       "       [ 1.29836385, -1.22788604],\n",
       "       [-0.45411379,  1.56351929],\n",
       "       [-0.4502812 ,  0.05959701],\n",
       "       [-0.97691117,  1.14399935],\n",
       "       [-0.25188652,  0.45561419],\n",
       "       [-2.94335955, -1.32607226],\n",
       "       [-0.17755661, -1.88561599],\n",
       "       [ 0.25805053, -0.82906481],\n",
       "       [ 0.79731266,  0.41474386],\n",
       "       [-0.95375292,  1.2805051 ],\n",
       "       [-2.38961976, -2.86154405],\n",
       "       [ 0.45015399,  0.30274369],\n",
       "       [-0.42365902,  0.24263656],\n",
       "       [-1.63066748,  0.59939772],\n",
       "       [ 0.91519053,  0.23408284],\n",
       "       [ 0.41881822,  1.43182747],\n",
       "       [-0.56572464, -0.76404521],\n",
       "       [ 0.66096031,  0.38188795],\n",
       "       [ 2.21978036,  0.95490367],\n",
       "       [ 1.95886788,  0.14773   ],\n",
       "       [-0.71878901, -0.53117814],\n",
       "       [-2.61149778, -0.1134759 ],\n",
       "       [-0.26995871,  0.56751654],\n",
       "       [ 1.40862185,  0.59309577],\n",
       "       [ 0.25959953, -0.12010371],\n",
       "       [ 0.24177621,  0.26524183],\n",
       "       [-1.68494295, -1.40254275],\n",
       "       [ 0.84269863, -0.26146648],\n",
       "       [-0.93219377,  0.21304251],\n",
       "       [-0.25405407,  0.60921077],\n",
       "       [ 1.14252149, -2.04185606],\n",
       "       [ 0.5505287 ,  0.61148851],\n",
       "       [-0.01976046, -0.26782993],\n",
       "       [-0.81700013,  0.7062792 ],\n",
       "       [-0.93449455,  1.7106801 ],\n",
       "       [ 1.23706408, -2.11698291],\n",
       "       [-1.86876733, -0.70172097],\n",
       "       [ 1.164155  ,  0.29961777],\n",
       "       [ 0.39672484, -0.43994323],\n",
       "       [-0.72062728,  0.00506879],\n",
       "       [ 1.96732375, -0.6216896 ],\n",
       "       [ 0.86732499,  0.45412527],\n",
       "       [ 0.16130355,  1.03126981],\n",
       "       [ 1.21460828, -0.82259977],\n",
       "       [-0.5785441 ,  1.34858558],\n",
       "       [-0.68901268,  0.22823286],\n",
       "       [-0.60310195, -1.32865364],\n",
       "       [ 2.1917096 , -0.46270522],\n",
       "       [ 0.55476981, -2.54964851],\n",
       "       [ 0.2802618 ,  0.06567881],\n",
       "       [ 1.36403286,  2.25115694],\n",
       "       [-1.57984097,  0.51323655],\n",
       "       [-2.60526762, -1.42811285],\n",
       "       [ 0.54985726, -0.22851355],\n",
       "       [ 1.38132812, -0.42885646],\n",
       "       [ 2.69554336, -0.5369392 ],\n",
       "       [-0.50255396, -2.41655255],\n",
       "       [ 0.8931933 ,  0.95012258],\n",
       "       [ 2.36112455, -0.189118  ],\n",
       "       [ 0.15590728,  1.72635454],\n",
       "       [-0.73451281,  1.22576857],\n",
       "       [ 0.87223935, -1.43935804],\n",
       "       [-1.03067081,  0.13748594],\n",
       "       [-0.72738008, -0.44817973],\n",
       "       [ 1.05665383,  1.08237882],\n",
       "       [ 1.06003857, -1.18998714],\n",
       "       [-0.13881112, -0.19849837],\n",
       "       [-0.55222862,  0.83267996],\n",
       "       [ 2.57897482,  0.21052826],\n",
       "       [-1.26977601, -0.21419395],\n",
       "       [-1.55602796, -0.07782508],\n",
       "       [ 0.10024889, -0.4130332 ],\n",
       "       [ 0.03854246, -2.67768459],\n",
       "       [ 1.91783848,  0.00837207],\n",
       "       [ 0.96095907, -1.33931808],\n",
       "       [-0.91958826, -1.00234253],\n",
       "       [-0.79873037, -1.60721358],\n",
       "       [ 1.35990689,  1.68073735],\n",
       "       [ 0.82400831,  0.2473152 ],\n",
       "       [-1.84709633,  0.05922256],\n",
       "       [ 0.24021814,  0.37640878],\n",
       "       [ 1.41769019,  0.49671551],\n",
       "       [ 1.0895941 , -1.6465918 ],\n",
       "       [ 1.83288526, -0.38136659],\n",
       "       [ 1.81516738,  0.54874301],\n",
       "       [-1.42754581, -0.22905211],\n",
       "       [-0.86964042,  1.8476743 ],\n",
       "       [ 2.32649531, -0.08197772],\n",
       "       [ 0.901419  ,  2.85065645],\n",
       "       [-2.39249706, -0.49485268],\n",
       "       [ 0.24337989, -0.63592446],\n",
       "       [-1.93934006,  0.80875863],\n",
       "       [-1.82570021, -1.14449821],\n",
       "       [-0.70352872, -2.82855567],\n",
       "       [ 0.0393488 , -0.98827223],\n",
       "       [ 0.68461629, -0.1173614 ],\n",
       "       [ 0.05394739, -1.91324727],\n",
       "       [-2.19719559,  1.45438474],\n",
       "       [-2.74847125, -0.23299121],\n",
       "       [ 1.88335006,  0.94453176],\n",
       "       [ 0.56272462, -1.02020647],\n",
       "       [ 0.5182885 , -0.38656959],\n",
       "       [ 0.51677168, -0.47181302],\n",
       "       [ 1.8584757 , -2.65869091],\n",
       "       [-1.32701326,  1.29729286],\n",
       "       [-2.8565645 ,  0.96347738],\n",
       "       [-2.85954509, -0.18072299],\n",
       "       [-2.15796424, -2.53622651],\n",
       "       [ 0.30257107, -1.19144546],\n",
       "       [ 0.54213215, -0.14711832],\n",
       "       [ 2.15013021, -0.67366943],\n",
       "       [-1.89702776, -0.78735869],\n",
       "       [ 1.17308568, -1.36802517],\n",
       "       [-0.11800518,  1.03209744],\n",
       "       [ 0.58350575,  0.28752196],\n",
       "       [ 0.97138471, -0.61130258],\n",
       "       [-1.82958794, -0.2809939 ],\n",
       "       [ 2.30603724,  1.00477388],\n",
       "       [-1.78706556,  0.07866935],\n",
       "       [-0.5414861 , -1.03261248],\n",
       "       [-0.97249283,  2.66359035],\n",
       "       [-0.77978749, -0.61927617],\n",
       "       [-1.04034841,  1.92280154],\n",
       "       [-0.570706  ,  1.30053341],\n",
       "       [-0.19399486,  0.21153538],\n",
       "       [ 0.86817495, -0.01799267],\n",
       "       [ 0.72036824,  0.0312988 ],\n",
       "       [ 0.93909046,  0.11242812],\n",
       "       [-0.89214421,  0.47137032],\n",
       "       [-0.31003723, -0.61670629],\n",
       "       [ 1.37863868,  0.99749376],\n",
       "       [-0.04396467, -0.88007405],\n",
       "       [-1.28533911,  1.51224409],\n",
       "       [ 2.84199877, -0.38748532],\n",
       "       [-2.03157017,  2.33978071],\n",
       "       [-1.57645184, -0.69290038],\n",
       "       [ 0.52680301, -1.83069725],\n",
       "       [ 1.01972965,  0.85339516],\n",
       "       [ 3.05865879,  0.04171176],\n",
       "       [-0.97655637, -0.81640838],\n",
       "       [-2.28444865, -0.77583618],\n",
       "       [-0.53080039, -0.83520457],\n",
       "       [ 0.07670867,  0.42357019],\n",
       "       [-0.21856281, -0.5005899 ],\n",
       "       [-1.52368217, -0.19323049],\n",
       "       [-0.97775648, -0.8786859 ],\n",
       "       [-0.36036553, -1.03196818],\n",
       "       [-0.45450977,  0.05299519],\n",
       "       [ 0.66564396,  1.33878159],\n",
       "       [-0.54691332,  0.00732991],\n",
       "       [-1.84264744, -0.98824581],\n",
       "       [-0.96611966, -1.96561566],\n",
       "       [ 0.40307366,  0.3062482 ],\n",
       "       [-1.48541761, -0.48533685],\n",
       "       [-0.684893  ,  1.66483914],\n",
       "       [-0.01904605, -0.83195387],\n",
       "       [ 0.50320323, -2.23071564],\n",
       "       [-0.03038629, -0.10255469],\n",
       "       [-1.46940975, -0.67720247],\n",
       "       [ 0.09681572,  0.0168135 ],\n",
       "       [ 1.70248482, -0.31645321],\n",
       "       [ 0.15600372,  0.99174486],\n",
       "       [-0.18160557, -0.27525565],\n",
       "       [-0.91803614, -0.896282  ],\n",
       "       [-1.03573104, -0.80428999],\n",
       "       [-0.87239323,  0.8523482 ],\n",
       "       [-0.35040776,  0.33758549],\n",
       "       [-0.36644652, -1.47204783],\n",
       "       [ 0.75686692, -1.20061614],\n",
       "       [ 1.70660205,  1.27828032],\n",
       "       [-0.01817219, -0.32822819],\n",
       "       [-2.14635059,  2.09434942],\n",
       "       [ 0.20807923, -0.42135194],\n",
       "       [-1.00861876,  1.35601094],\n",
       "       [-0.16319738,  1.01129651],\n",
       "       [-1.63035741,  1.22874455],\n",
       "       [ 0.53707442,  0.77601747],\n",
       "       [ 1.18089115,  0.72571133],\n",
       "       [-0.68835241,  0.51673106],\n",
       "       [-1.13742318, -0.61412004],\n",
       "       [-0.43402776,  0.89377738],\n",
       "       [ 2.34259389,  1.31457714],\n",
       "       [ 0.88411387,  2.27142982],\n",
       "       [-1.39885139, -0.64930323],\n",
       "       [-0.28207227,  0.78820936],\n",
       "       [ 0.39357453,  2.26449121],\n",
       "       [-1.67864621, -1.30564223],\n",
       "       [-0.90076282,  2.47250627],\n",
       "       [ 3.17232027,  0.71339094],\n",
       "       [ 1.55572707,  0.22555045],\n",
       "       [ 1.48207966, -1.22026815],\n",
       "       [ 1.64089978, -1.10576288],\n",
       "       [-2.23991801,  1.33048759],\n",
       "       [-0.76971811,  2.1431704 ],\n",
       "       [-1.21917671, -1.50826204],\n",
       "       [ 1.71795671, -0.69458207],\n",
       "       [-0.87581354,  1.25670903],\n",
       "       [ 0.92905863, -0.72608552],\n",
       "       [ 0.22579019,  1.07389389],\n",
       "       [-0.71718566,  2.67904644],\n",
       "       [-0.33318838,  1.29307878],\n",
       "       [-0.14136622,  0.88163948],\n",
       "       [-0.7140613 , -0.47223969],\n",
       "       [ 1.31980175, -0.37571691],\n",
       "       [ 0.39456192, -1.62866209],\n",
       "       [-0.58565261, -0.11170289],\n",
       "       [ 0.60319809,  2.21163587],\n",
       "       [-0.22986907, -0.68682849],\n",
       "       [ 1.48149401,  0.30483369],\n",
       "       [-0.90044869,  1.64424106],\n",
       "       [ 0.18524242,  0.59336831],\n",
       "       [ 0.45484619, -0.53566512],\n",
       "       [-1.04040251,  0.43840026],\n",
       "       [-1.26460352, -0.35310049],\n",
       "       [-0.4920401 , -1.30844684],\n",
       "       [-1.76003544,  0.33805953],\n",
       "       [-0.8564291 ,  1.10610037],\n",
       "       [-1.00194934,  1.48650268],\n",
       "       [-0.87640029,  0.93988496],\n",
       "       [-0.61002537,  0.78409691],\n",
       "       [ 0.04083412,  0.59817336],\n",
       "       [ 0.41425144, -1.21011871],\n",
       "       [-0.41821333,  0.3569954 ],\n",
       "       [ 1.19671389, -0.08489054],\n",
       "       [ 1.6619406 , -0.1531252 ],\n",
       "       [ 0.9753225 ,  0.09235337],\n",
       "       [ 0.34172475,  0.21981538],\n",
       "       [ 0.43899858,  1.01855188],\n",
       "       [-1.3150825 ,  1.80149571],\n",
       "       [ 0.87908231, -0.63505423],\n",
       "       [ 0.04535077,  2.76087664]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**探索性分析**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16, train16, sample_submission = load_original()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's watch these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train16.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_submission.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Merge train and feature to the whole train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train16, properties16, on='parcelid', how='left')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are so many NaN in features.  \n",
    "We consider mean value inplace origial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_values = train_df.mean(axis=0)\n",
    "train_df_new = train_df.fillna(mean_values, inplace=True)\n",
    "train_df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Univariate Analysis:\n",
    "Since there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation coefficient: feature VS. label #\n",
    "x_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0, 1])\n",
    "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n",
    "corr_df = corr_df.sort_values(by='corr_values')\n",
    "\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation coefficient of the variables\")\n",
    "#autolabel(rects)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "select the feature with top correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n",
    "corr_df_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let us look at the correlation coefficient of each of these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation coefficient: feature 1 VS. feature 2 #\n",
    "cols_to_use = corr_df_sel.col_labels.tolist()\n",
    "\n",
    "temp_df = train_df[cols_to_use]\n",
    "corrmat = temp_df.corr(method='spearman')\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(corrmat, vmax=1., square=True)\n",
    "plt.title(\"Important variables correlation map\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "* Base classifiers\n",
    "* Ensemble classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Firstly, we need to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test, columns = load_supervision()\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change dtypes of features #\n",
    "# train_df_new['hashottuborspa'].fillna(0, inplace=True)\n",
    "# train_df_new['hashottuborspa'][train_df_new['hashottuborspa'] == True] = 1\n",
    "\n",
    "# en_pro = LabelEncoder()\n",
    "# train_df_new['propertyzoningdesc'].fillna('Unknown', inplace=True)\n",
    "# train_df_new['propertyzoningdesc'] = en_pro.fit_transform(train_df_new['propertyzoningdesc'])\n",
    "\n",
    "# en_fire = LabelEncoder()\n",
    "# train_df_new['fireplaceflag'].fillna(0, inplace=True)\n",
    "# train_df_new['fireplaceflag'][train_df_new['fireplaceflag'] == True] = 1\n",
    "\n",
    "# en_tax = LabelEncoder()\n",
    "# train_df_new['taxdelinquencyflag'].fillna(0, inplace=True)\n",
    "# train_df_new['taxdelinquencyflag'][train_df_new['taxdelinquencyflag'] == 'Y'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Base Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We consider the following models (or base learners)\n",
    "gb = xgb.XGBRegressor(nthread=1, seed=SEED)\n",
    "ls = linear_model.Lasso(alpha=1e-6, normalize=True)\n",
    "el = linear_model.ElasticNet(alpha=1e-6, normalize=True)\n",
    "rf = ensemble.RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "base_learners = [('ls', ls), ('el', el), ('rf', rf), ('gb', gb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "P = np.zeros((X_valid.shape[0], len(base_learners)))\n",
    "P = pd.DataFrame(P, columns=[e for e, _ in base_learners])\n",
    "\n",
    "for est_name, est in base_learners:\n",
    "    est.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = est.predict(X_valid)\n",
    "    P.loc[:, est_name] = y_pred\n",
    "    print(\"%3s : %.4f\" % (est_name, metrics.mean_absolute_error(y_valid.values.ravel(), y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualize the performance of ensemble classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corrmat(P.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compare Base Classifiers, search the best params of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Put their parameter dictionaries in a dictionary with the\n",
    "# estimator names as keys\n",
    "param_dicts = {'ls':\n",
    "                  {'alpha': uniform(1e-6, 1e-5)},\n",
    "               'el':\n",
    "                  {'alpha': uniform(1e-6, 1e-5),\n",
    "                   'l1_ratio': uniform(0, 1)\n",
    "                  },\n",
    "               'gb':\n",
    "                   {'learning_rate': uniform(0.02, 0.04),\n",
    "                    'colsample_bytree': uniform(0.55, 0.66),\n",
    "                    'min_child_weight': randint(30, 60),\n",
    "                    'max_depth': randint(3, 7),\n",
    "                    'subsample': uniform(0.4, 0.2),\n",
    "                    'n_estimators': randint(150, 200),\n",
    "                    'colsample_bytree': uniform(0.6, 0.4),\n",
    "                    'reg_lambda': uniform(1, 2),\n",
    "                    'reg_alpha': uniform(1, 2),\n",
    "                   },\n",
    "               'rf':\n",
    "                   {'max_depth': randint(2, 5),\n",
    "                    'min_samples_split': randint(5, 20),\n",
    "                    'min_samples_leaf': randint(10, 20),\n",
    "                    'n_estimators': randint(50, 100),\n",
    "                    'max_features': uniform(0.6, 0.3)}\n",
    "              }\n",
    "\n",
    "scorer = make_scorer(metrics.mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "evl = Evaluator(scorer, cv=2, random_state=SEED, verbose=5)\n",
    "\n",
    "evl.fit(X_train.values, \n",
    "        y_train.values.ravel(), \n",
    "        estimators=base_learners, \n",
    "        param_dicts=param_dicts, \n",
    "        preprocessing={'sc': [preprocessing.StandardScaler()], 'none': []},\n",
    "        n_iter=2)  # bump this up to do a larger grid search\n",
    "\n",
    "pd.DataFrame(evl.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model selection guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # set the best params\n",
    "# for case_name, params in evl.summary[\"params\"].items():\n",
    "#     for est_name, est in base_learners:\n",
    "#         if est_name == case_name[1]:\n",
    "#             est.set_params(**params)\n",
    "\n",
    "# We consider the following models (or base learners)\n",
    "LinearRegression = linear_model.LinearRegression()\n",
    "Lasso = linear_model.Lasso(alpha=1e-6, normalize=True)\n",
    "ElasticNet = linear_model.ElasticNet(alpha=1e-6, normalize=True)\n",
    "RandomForest = ensemble.RandomForestRegressor(random_state=SEED)\n",
    "XGBoost = xgb.XGBRegressor(nthread=1, seed=SEED)\n",
    "AdaBoost = ensemble.AdaBoostRegressor(random_state=SEED)\n",
    "Bagging = ensemble.BaggingRegressor(random_state=SEED)\n",
    "\n",
    "base_learners = [('Lasso', Lasso), \n",
    "                 ('ElasticNet', ElasticNet), \n",
    "                 ('RandomForest', RandomForest),\n",
    "                 ('XGBoost', XGBoost),\n",
    "                 ('AdaBoost', AdaBoost),\n",
    "                 ('Bagging', Bagging),\n",
    "                ]\n",
    "\n",
    "\n",
    "# define meta learners\n",
    "meta_learners = [('XGBoost', XGBoost), \n",
    "                 ('ElasticNet', ElasticNet),\n",
    "                 ('LinearRegression', LinearRegression),\n",
    "                ]\n",
    "\n",
    "# Note that when we have a preprocessing pipeline,\n",
    "# keys are in the (prep_name, est_name) format\n",
    "param_dicts = {'ElasticNet':\n",
    "                  {'alpha': uniform(1e-5, 1),\n",
    "                   'l1_ratio': uniform(0, 1)},\n",
    "               'XGBoost':\n",
    "                   {'learning_rate': uniform(0.01, 0.2),\n",
    "                    'subsample': uniform(0.5, 0.5),\n",
    "                    'reg_lambda': uniform(0.1, 1),\n",
    "                    'n_estimators': randint(10, 100)},\n",
    "              }\n",
    "\n",
    "# ensemble base learners\n",
    "proba_transformer = EnsembleTransformer().add('stack', base_learners, proba=False)\n",
    "class_transformer = EnsembleTransformer().add('blend', base_learners, proba=False)\n",
    "preprocessing = {'stack': [('layer-1', proba_transformer)],\n",
    "                 'blend': [('layer-1', class_transformer)]}\n",
    "\n",
    "# new Evaluator\n",
    "scorer = make_scorer(metrics.mean_absolute_error, greater_is_better=False)\n",
    "evl = Evaluator(scorer, cv=2, random_state=SEED, verbose=5)\n",
    "\n",
    "# train meta_learners with wrapped base learners, .values\n",
    "evl.fit(X_train,\n",
    "        y_train,\n",
    "        meta_learners,\n",
    "        param_dicts,\n",
    "        preprocessing=preprocessing,\n",
    "        n_iter=20)    # bump this up to do a larger grid search\n",
    "\n",
    "meta_summary = pd.DataFrame(evl.summary)\n",
    "meta_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 集成学习（开源实现）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, columns = load_supervision()\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test = X_train.values, y_train.values, X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "clf_first_layer = [('RandomForest', ensemble.RandomForestRegressor()), \n",
    "                   # ('AdaBoost', ensemble.AdaBoostRegressor()), \n",
    "                   # ('XGBoost', xgb.XGBRegressor()), \n",
    "                   ('GBDT', ensemble.GradientBoostingRegressor()), \n",
    "                   ('Bagging', ensemble.BaggingRegressor()), \n",
    "                   ('ExtraTrees', ensemble.ExtraTreesRegressor()),\n",
    "                   ('ElasticNet', linear_model.ElasticNet()),\n",
    "                   ('Huber', linear_model.HuberRegressor()),\n",
    "                   ('BayesianRidge', linear_model.BayesianRidge()),\n",
    "                   ('Lasso', linear_model.Lasso()),\n",
    "                   ('LassoLars', linear_model.LassoLars()),\n",
    "                   ('LinearRegression', linear_model.LinearRegression()),\n",
    "                  ]\n",
    "\n",
    "clf_secon_layer = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# meta_learner = meta_learners[1][1]\n",
    "# meta_learner.set_params(**evl.summary[\"params\"][(\"meta\", \"el\")])\n",
    "\n",
    "ens = SuperLearner(folds=5, verbose=5)\n",
    "ens.add(clf_first_layer)  # SequentialEnsemble: 0.0633532638459\n",
    "ens.add(clf_first_layer)  # SequentialEnsemble: 0.0633258630442, SuperLearner cv 5: 0.0611558872921\n",
    "ens.add(clf_first_layer)  # SuperLearner cv 5: 0.0604582704967\n",
    "ens.add(clf_secon_layer, meta=True)\n",
    "\n",
    "ens.fit(X, y)\n",
    "\n",
    "y_pred = ens.predict(X)\n",
    "print \"mean_absolute_error: \", metrics.mean_absolute_error(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = ens.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "print \"Writing ...\"\n",
    "sub = pd.read_csv('Data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "sub.to_csv('ensemble.csv', index=False, float_format='%.4f')\n",
    "\n",
    "print \"Zipping ...\"\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\") as fout:\n",
    "    fout.write(\"ensemble.csv\", compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# summary = pd.DataFrame(evl.summary)\n",
    "# writer = pd.ExcelWriter('output.xlsx')\n",
    "# summary.to_excel(writer,'Sheet1')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 集成学习(自己的实现)\n",
    "\n",
    "1. 双层集成学习  \n",
    "2. 使用尽可能多的分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, columns = load_supervision()\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test = X_train.values, y_train.values, X_test.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def base_layer(X, y, clf_base_layer, cv=10):\n",
    "    train_sec = np.zeros((len(y), len(clf_base_layer)))\n",
    "    \n",
    "    for i, (clf_name, clf) in enumerate(clf_base_layer):\n",
    "        print time.ctime(),  'first layer classifier:', clf_name\n",
    "        # cross validation\n",
    "        y_score = cross_val_predict(clf, X, y, cv=cv, verbose=5, method='predict', n_jobs=-1)\n",
    "        train_sec[:, i] = y_score\n",
    "        print 'mean_absolute_error: ', metrics.mean_absolute_error(y, y_score)\n",
    "    return train_sec\n",
    "\n",
    "\n",
    "def base_layer_pred(X, y, clf_base_layer, X_test, cv=10):\n",
    "    train_sec = np.zeros((len(y), len(clf_base_layer)))\n",
    "    train_pre = np.zeros((len(X_test), len(clf_base_layer)))\n",
    "    \n",
    "    for i, (clf_name, clf) in enumerate(clf_base_layer):\n",
    "        print time.ctime(),  'first layer classifier:', clf_name\n",
    "        # fit and predict\n",
    "        clf.n_jobs = -1\n",
    "        clf.fit(X, y)\n",
    "        train_sec[:, i], train_pre[:, i] = clf.predict(X), clf.predict(X_test)\n",
    "    return train_sec, train_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train\n",
    "train_fea = base_layer(X, y, clf_first_layer)\n",
    "train_fea = base_layer(train_fea, y, clf_first_layer)\n",
    "\n",
    "print time.ctime(), 'second layer classifier:', 'LinearRegression'\n",
    "y_score = cross_val_predict(clf_secon_layer, train_fea, y, cv=10, verbose=5, method='predict', n_jobs=-1)\n",
    "print 'mean_absolute_error: ', metrics.mean_absolute_error(y, y_score)\n",
    "\n",
    "pd.DataFrame(train_fea).to_csv('ensemble_diy.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test\n",
    "test_fea, test_pre = base_layer_pred(X, y, clf_first_layer, X_test)\n",
    "test_fea, test_pre = base_layer_pred(test_fea, y, clf_first_layer, test_pre)\n",
    "\n",
    "# second layer\n",
    "print time.ctime(), 'second layer classifier:', 'LinearRegression'\n",
    "clf_secon_layer.n_jobs = -1\n",
    "y_pred = clf_secon_layer.fit(test_fea, y).predict(test_pre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "print \"Writing ...\"\n",
    "sub = pd.read_csv('Data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "sub.to_csv('ensemble_diy.csv', index=False, float_format='%.4f')\n",
    "\n",
    "print \"Zipping ...\"\n",
    "with zipfile.ZipFile(\"ensemble_diy.zip\", \"w\") as fout:\n",
    "    fout.write(\"ensemble_diy.csv\", compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Genetic Programming （遗传算法/进化规划）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = X.values, y.values.ravel()\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "X_train = standardScaler.fit_transform(X_train)\n",
    "X_test = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Genetic Programming\n",
    "from gplearn.skutils import check_random_state\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# train GP\n",
    "function_set = ['add','sub','mul','div','sqrt','log','abs','neg','inv','max','min','sin','cos','tan']\n",
    "\n",
    "titan_gp = SymbolicRegressor(population_size=2000,\n",
    "                             generations=20, \n",
    "                             stopping_criteria=0.06774,\n",
    "                             p_crossover=0.7, \n",
    "                             p_subtree_mutation=0.1,\n",
    "                             p_hoist_mutation=0.05, \n",
    "                             p_point_mutation=0.1,\n",
    "                             max_samples=0.9, \n",
    "                             verbose=1,\n",
    "                             parsimony_coefficient=0.01,\n",
    "                             function_set=function_set,\n",
    "                             n_jobs=1,\n",
    "                             random_state=2017)\n",
    "titan_gp.fit(X_train, y_train) # np.array([np.random.randint(2) for i in range(len(y_train))])\n",
    "\n",
    "# print relationship\n",
    "print 'Relationship:', titan_gp._program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = titan_gp.predict(X_test)\n",
    "\n",
    "sub = pd.read_csv('Data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "sub.to_csv('gplearn.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 标签传播算法\n",
    "\n",
    "1. 标签传播算法只能处理分类问题。这里先将标签去头去尾，再取其小数值构成少量标签。目的是将未标签的值标上标签，增加训练集样本。  \n",
    "2. 不考虑交易日期属性：删除原训练集中的parcelid重复项，与属性集融合，构成一个半监督数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16, train16, sample_submission = load_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_test = train16.copy()\n",
    "\n",
    "ulimit = np.percentile(train_test.logerror.values, 99)\n",
    "llimit = np.percentile(train_test.logerror.values, 1)\n",
    "train_test['logerror'].ix[train_test['logerror']>ulimit] = ulimit\n",
    "train_test['logerror'].ix[train_test['logerror']<llimit] = llimit\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.distplot(train_test.logerror.values, kde=True)\n",
    "plt.xlabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "logerror_short = np.array([Decimal(str(i)).quantize(Decimal('0.0')) for i in np.sort(train_test.logerror.values)], dtype=np.float)\n",
    "\n",
    "le = LabelEncoder()\n",
    "logerror_short_label = le.fit_transform(logerror_short)\n",
    "\n",
    "train_test['logerror'] = logerror_short_label\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.distplot(train_test.logerror.values, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "对属性值做PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_proper():\n",
    "    properties = properties16.copy()\n",
    "\n",
    "    properties = properties.drop(['propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "\n",
    "    properties.fillna(0, inplace=True)\n",
    "\n",
    "    convert = properties.dtypes == 'object'\n",
    "    properties.loc[:, convert] = properties.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    properties_pca = pca.fit_transform(properties)\n",
    "    \n",
    "    properties_pca = pd.DataFrame(properties_pca)\n",
    "    properties_pca['parcelid'] = properties16['parcelid']\n",
    "    \n",
    "    return pd.DataFrame(properties_pca)\n",
    "\n",
    "\n",
    "train = pd.merge(train_test.drop_duplicates('parcelid'), build_proper(), on=\"parcelid\", how=\"outer\")\n",
    "\n",
    "# 将logerror为NaN的值换成-1\n",
    "train['logerror'].fillna(-1, inplace=True)\n",
    "\n",
    "y = train.logerror\n",
    "train = train.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "\n",
    "for c, dtype in zip(train.columns, train.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "    if dtype == np.int64:\n",
    "        train[c] = train[c].astype(np.int8)\n",
    "\n",
    "train = train.values\n",
    "y = y.values\n",
    "\n",
    "print train.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_ss = ss.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ss, y, test_size=0.005, random_state=SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "\n",
    "# skf = StratifiedKFold(y, n_folds=10)\n",
    "# for train_index, test_index in skf:\n",
    "#     X_train, X_test = train_ss[train_index], train_ss[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     print(\"=== TRAIN:\", X_train.shape, \"TEST:\", X_test.shape)\n",
    "    \n",
    "#     skf2 = StratifiedKFold(y_test, n_folds=5)\n",
    "#     for train_index2, test_index2 in skf2:\n",
    "#         X_train2, X_test2 = X_test[train_index2], X_test[test_index2]\n",
    "#         y_train2, y_test2 = y_test[train_index2], y_test[test_index2]\n",
    "#         print(\"TRAIN2:\", X_train2.shape, \"TEST2:\", X_test2.shape)\n",
    "#         ls.fit(X_train2, y_train2)\n",
    "#         y_pred2 = ls.predict(X_test2)\n",
    "#         print metrics.classification_report(y_test2, y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ls = LabelSpreading()\n",
    "\n",
    "ls.fit(X_train, y_train)\n",
    "y_pred = ls.predict(X_test)\n",
    "# print metrics.classification_report(y_test, y_pred)\n",
    "pd.DataFrame({\"True\": y_test, \"Pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
