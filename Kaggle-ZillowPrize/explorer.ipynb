{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Zillow Prediction\n",
    "\n",
    "* dtypes, fillnull, datatime\n",
    "* 相关性分析，去除不相关的特征\n",
    "* 确定问题是回归问题\n",
    "* 选择模型直接的相关度，进行融合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Basic packages\n",
    "import csv\n",
    "import time\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.options.display.max_columns = 999\n",
    "\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "\n",
    "# Model Persistence\n",
    "import cPickle\n",
    "\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn import model_selection, preprocessing, cross_validation, metrics\n",
    "from sklearn import ensemble, linear_model\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.model_selection import cross_val_score,cross_val_predict, train_test_split, StratifiedKFold\n",
    "from sklearn.neural_network import BernoulliRBM, MLPClassifier \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import f1_score, scorer, make_scorer\n",
    "\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# Ensemble Learning\n",
    "from mlens.ensemble import SuperLearner\n",
    "from mlens.metrics import make_scorer\n",
    "from mlens.model_selection import Evaluator\n",
    "from mlens.preprocessing import EnsembleTransformer\n",
    "\n",
    "from mlens.visualization import corr_X_y, corrmat\n",
    "\n",
    "\n",
    "# Imbalanced Learning\n",
    "import imblearn\n",
    "from imblearn.combine import SMOTETomek\n",
    "\n",
    "\n",
    "# Count\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Random Seed\n",
    "SEED = 2017\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "载入原始数据集，保留所有的数据特点，\n",
    "用于特征工程分析、半监督学习\n",
    "\"\"\"\n",
    "def load_original():\n",
    "    # u'parcelid', ...... (total 58 columns)\n",
    "    properties16 = pd.read_csv('Data/properties_2016.csv')             # 特征数据集，无重复parcelid\n",
    "    # u'parcelid', u'logerror', u'transactiondate\n",
    "    train16 = pd.read_csv('Data/train_2016_v2.csv', parse_dates=[\"transactiondate\"])   # 训练数据集，无重复行,但有重复parcelid\n",
    "    # u'ParcelId', u'201610', u'201611', u'201612', u'201710', u'201711', u'201712'\n",
    "    sample_submission = pd.read_csv('Data/sample_submission.csv')      # 测试数据集，无重复ParcelId\n",
    "\n",
    "    print 'properties16.shape\\t', properties16.shape\n",
    "    print 'train16.shape\\t\\t', train16.shape\n",
    "    print 'sample_submission.shape\\t', sample_submission.shape\n",
    "    \n",
    "    return properties16, train16, sample_submission\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "载入训练数据集，放弃特征数据集中的无标签/logerror数据，\n",
    "用于有监督学习\n",
    "\"\"\"\n",
    "def load_supervision():\n",
    "    \"\"\"Read in training data and return input, output, columns tuple.\"\"\"\n",
    "\n",
    "    # This is a version of Anovas minimally prepared dataset\n",
    "    # for the xgbstarter script\n",
    "    # https://www.kaggle.com/anokas/simple-xgboost-starter-0-0655\n",
    "\n",
    "    df = pd.read_csv('Data/train_2016_v2.csv')\n",
    "\n",
    "    prop = pd.read_csv('Data/properties_2016.csv')\n",
    "    convert = prop.dtypes == 'float64'\n",
    "    prop.loc[:, convert] = prop.loc[:, convert].apply(lambda x: x.astype(np.float32))\n",
    "\n",
    "    df = df.merge(prop, how='left', on='parcelid')\n",
    "\n",
    "    y = df.logerror\n",
    "    df = df.drop(['parcelid', 'logerror', 'transactiondate', 'propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "\n",
    "    convert = df.dtypes == 'object'\n",
    "    df.loc[:, convert] = df.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "\n",
    "    df.fillna(0, inplace=True)\n",
    "    \n",
    "    # test\n",
    "    sample = pd.read_csv('Data/sample_submission.csv')\n",
    "    sample['parcelid'] = sample['ParcelId']\n",
    "    sample = sample[['parcelid']].merge(prop, how='left', on='parcelid')\n",
    "    sample = sample.drop(['parcelid', 'propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "    \n",
    "    convert = sample.dtypes == 'object'\n",
    "    sample.loc[:, convert] = sample.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "    \n",
    "    sample.fillna(0, inplace=True)\n",
    "    \n",
    "    return df, y, sample, df.columns\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "输出二分类评估结果\n",
    "\"\"\"\n",
    "def binary_label_report(y_true, y_pred):\n",
    "    print 'f1_score:\\t\\t', metrics.f1_score(y_true, y_pred)\n",
    "    print 'accuracy_score:\\t\\t', metrics.accuracy_score(y_true, y_pred)\n",
    "    print 'matthews_corrcoef:\\t', metrics.matthews_corrcoef(y_true, y_pred)\n",
    "    print 'roc_auc_score:\\t\\t', metrics.roc_auc_score(y_true, y_pred)\n",
    "    \n",
    "\n",
    "\"\"\"\n",
    "输出多标签分类评估结果\n",
    "\"\"\"\n",
    "def multi_label_report(y_true, y_pred, y_score):\n",
    "    binary_label_report(y_true, y_pred)\n",
    "    print 'hamming_loss:\\t\\t', metrics.hamming_loss(y_true, y_pred)\n",
    "    # print 'hinge_loss:\\t\\t', metrics.hinge_loss(y_true, y_pred)\n",
    "    # print 'coverage_error:\\t\\t', metrics.coverage_error(y_true, y_pred)\n",
    "    print 'average_precision_score (micro):\\t\\t', metrics.average_precision_score(y_true, y_score, average='micro')\n",
    "    print 'average_precision_score (macro):\\t\\t', metrics.average_precision_score(y_true, y_score, average='macro')\n",
    "    print 'average_precision_score (weighted):\\t\\t', metrics.average_precision_score(y_true, y_score, average='weighted')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16, train16, sample_submission = load_original()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's watch these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train16.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sample_submission.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Merge train and feature to the whole train file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train16, properties16, on='parcelid', how='left')\n",
    "train_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "There are so many NaN in features.  \n",
    "We consider mean value inplace origial value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "mean_values = train_df.mean(axis=0)\n",
    "train_df_new = train_df.fillna(mean_values, inplace=True)\n",
    "train_df_new.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_df_new.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Univariate Analysis:\n",
    "Since there are so many variables, let us first take the 'float' variables alone and then get the correlation with the target variable to see how they are related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation coefficient: feature VS. label #\n",
    "x_cols = [col for col in train_df_new.columns if col not in ['logerror'] if train_df_new[col].dtype=='float64']\n",
    "\n",
    "labels = []\n",
    "values = []\n",
    "for col in x_cols:\n",
    "    labels.append(col)\n",
    "    values.append(np.corrcoef(train_df_new[col].values, train_df_new.logerror.values)[0, 1])\n",
    "corr_df = pd.DataFrame({'col_labels':labels, 'corr_values':values})\n",
    "corr_df = corr_df.sort_values(by='corr_values')\n",
    "\n",
    "ind = np.arange(len(labels))\n",
    "width = 0.9\n",
    "fig, ax = plt.subplots(figsize=(10,20))\n",
    "rects = ax.barh(ind, np.array(corr_df.corr_values.values), color='y')\n",
    "ax.set_yticks(ind)\n",
    "ax.set_yticklabels(corr_df.col_labels.values, rotation='horizontal')\n",
    "ax.set_xlabel(\"Correlation coefficient\")\n",
    "ax.set_title(\"Correlation coefficient of the variables\")\n",
    "#autolabel(rects)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "select the feature with top correlation coefficient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corr_df_sel = corr_df.ix[(corr_df['corr_values']>0.02) | (corr_df['corr_values'] < -0.01)]\n",
    "corr_df_sel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let us look at the correlation coefficient of each of these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# correlation coefficient: feature 1 VS. feature 2 #\n",
    "cols_to_use = corr_df_sel.col_labels.tolist()\n",
    "\n",
    "temp_df = train_df[cols_to_use]\n",
    "corrmat = temp_df.corr(method='spearman')\n",
    "\n",
    "# Draw the heatmap using seaborn\n",
    "f, ax = plt.subplots(figsize=(8, 8))\n",
    "sns.heatmap(corrmat, vmax=1., square=True)\n",
    "plt.title(\"Important variables correlation map\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Machine Learning\n",
    "\n",
    "* Base classifiers\n",
    "* Ensemble classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Firstly, we need to load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X, y, X_test, columns = load_supervision()\n",
    "print 'X.shape:', X.shape\n",
    "print 'y.shape:', y.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change dtypes of features #\n",
    "# train_df_new['hashottuborspa'].fillna(0, inplace=True)\n",
    "# train_df_new['hashottuborspa'][train_df_new['hashottuborspa'] == True] = 1\n",
    "\n",
    "# en_pro = LabelEncoder()\n",
    "# train_df_new['propertyzoningdesc'].fillna('Unknown', inplace=True)\n",
    "# train_df_new['propertyzoningdesc'] = en_pro.fit_transform(train_df_new['propertyzoningdesc'])\n",
    "\n",
    "# en_fire = LabelEncoder()\n",
    "# train_df_new['fireplaceflag'].fillna(0, inplace=True)\n",
    "# train_df_new['fireplaceflag'][train_df_new['fireplaceflag'] == True] = 1\n",
    "\n",
    "# en_tax = LabelEncoder()\n",
    "# train_df_new['taxdelinquencyflag'].fillna(0, inplace=True)\n",
    "# train_df_new['taxdelinquencyflag'][train_df_new['taxdelinquencyflag'] == 'Y'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Base Classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We consider the following models (or base learners)\n",
    "gb = xgb.XGBRegressor(nthread=1, seed=SEED)\n",
    "ls = linear_model.Lasso(alpha=1e-6, normalize=True)\n",
    "el = linear_model.ElasticNet(alpha=1e-6, normalize=True)\n",
    "rf = ensemble.RandomForestRegressor(random_state=SEED)\n",
    "\n",
    "base_learners = [('ls', ls), ('el', el), ('rf', rf), ('gb', gb)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "P = np.zeros((X_valid.shape[0], len(base_learners)))\n",
    "P = pd.DataFrame(P, columns=[e for e, _ in base_learners])\n",
    "\n",
    "for est_name, est in base_learners:\n",
    "    est.fit(X_train, y_train.values.ravel())\n",
    "    y_pred = est.predict(X_valid)\n",
    "    P.loc[:, est_name] = y_pred\n",
    "    print(\"%3s : %.4f\" % (est_name, metrics.mean_absolute_error(y_valid.values.ravel(), y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Visualize the performance of ensemble classifiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "corrmat(P.corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compare Base Classifiers, search the best params of them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Put their parameter dictionaries in a dictionary with the\n",
    "# estimator names as keys\n",
    "param_dicts = {'ls':\n",
    "                  {'alpha': uniform(1e-6, 1e-5)},\n",
    "               'el':\n",
    "                  {'alpha': uniform(1e-6, 1e-5),\n",
    "                   'l1_ratio': uniform(0, 1)\n",
    "                  },\n",
    "               'gb':\n",
    "                   {'learning_rate': uniform(0.02, 0.04),\n",
    "                    'colsample_bytree': uniform(0.55, 0.66),\n",
    "                    'min_child_weight': randint(30, 60),\n",
    "                    'max_depth': randint(3, 7),\n",
    "                    'subsample': uniform(0.4, 0.2),\n",
    "                    'n_estimators': randint(150, 200),\n",
    "                    'colsample_bytree': uniform(0.6, 0.4),\n",
    "                    'reg_lambda': uniform(1, 2),\n",
    "                    'reg_alpha': uniform(1, 2),\n",
    "                   },\n",
    "               'rf':\n",
    "                   {'max_depth': randint(2, 5),\n",
    "                    'min_samples_split': randint(5, 20),\n",
    "                    'min_samples_leaf': randint(10, 20),\n",
    "                    'n_estimators': randint(50, 100),\n",
    "                    'max_features': uniform(0.6, 0.3)}\n",
    "              }\n",
    "\n",
    "scorer = make_scorer(metrics.mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "evl = Evaluator(scorer, cv=2, random_state=SEED, verbose=5)\n",
    "\n",
    "evl.fit(X_train.values, \n",
    "        y_train.values.ravel(), \n",
    "        estimators=base_learners, \n",
    "        param_dicts=param_dicts, \n",
    "        preprocessing={'sc': [preprocessing.StandardScaler()], 'none': []},\n",
    "        n_iter=2)  # bump this up to do a larger grid search\n",
    "\n",
    "pd.DataFrame(evl.summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Model selection guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# # set the best params\n",
    "# for case_name, params in evl.summary[\"params\"].items():\n",
    "#     for est_name, est in base_learners:\n",
    "#         if est_name == case_name[1]:\n",
    "#             est.set_params(**params)\n",
    "\n",
    "# We consider the following models (or base learners)\n",
    "LinearRegression = linear_model.LinearRegression()\n",
    "Lasso = linear_model.Lasso(alpha=1e-6, normalize=True)\n",
    "ElasticNet = linear_model.ElasticNet(alpha=1e-6, normalize=True)\n",
    "RandomForest = ensemble.RandomForestRegressor(random_state=SEED)\n",
    "XGBoost = xgb.XGBRegressor(nthread=1, seed=SEED)\n",
    "AdaBoost = ensemble.AdaBoostRegressor(random_state=SEED)\n",
    "Bagging = ensemble.BaggingRegressor(random_state=SEED)\n",
    "\n",
    "base_learners = [('Lasso', Lasso), \n",
    "                 ('ElasticNet', ElasticNet), \n",
    "                 ('RandomForest', RandomForest),\n",
    "                 ('XGBoost', XGBoost),\n",
    "                 ('AdaBoost', AdaBoost),\n",
    "                 ('Bagging', Bagging),\n",
    "                ]\n",
    "\n",
    "\n",
    "# define meta learners\n",
    "meta_learners = [('XGBoost', XGBoost), \n",
    "                 ('ElasticNet', ElasticNet),\n",
    "                 ('LinearRegression', LinearRegression),\n",
    "                ]\n",
    "\n",
    "# Note that when we have a preprocessing pipeline,\n",
    "# keys are in the (prep_name, est_name) format\n",
    "param_dicts = {'ElasticNet':\n",
    "                  {'alpha': uniform(1e-5, 1),\n",
    "                   'l1_ratio': uniform(0, 1)},\n",
    "               'XGBoost':\n",
    "                   {'learning_rate': uniform(0.01, 0.2),\n",
    "                    'subsample': uniform(0.5, 0.5),\n",
    "                    'reg_lambda': uniform(0.1, 1),\n",
    "                    'n_estimators': randint(10, 100)},\n",
    "              }\n",
    "\n",
    "# ensemble base learners\n",
    "proba_transformer = EnsembleTransformer().add('stack', base_learners, proba=False)\n",
    "class_transformer = EnsembleTransformer().add('blend', base_learners, proba=False)\n",
    "preprocessing = {'stack': [('layer-1', proba_transformer)],\n",
    "                 'blend': [('layer-1', class_transformer)]}\n",
    "\n",
    "# new Evaluator\n",
    "scorer = make_scorer(metrics.mean_absolute_error, greater_is_better=False)\n",
    "evl = Evaluator(scorer, cv=2, random_state=SEED, verbose=5)\n",
    "\n",
    "# train meta_learners with wrapped base learners, .values\n",
    "evl.fit(X_train,\n",
    "        y_train,\n",
    "        meta_learners,\n",
    "        param_dicts,\n",
    "        preprocessing=preprocessing,\n",
    "        n_iter=20)    # bump this up to do a larger grid search\n",
    "\n",
    "meta_summary = pd.DataFrame(evl.summary)\n",
    "meta_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Ensemble guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# meta_learner = meta_learners[1][1]\n",
    "# meta_learner.set_params(**evl.summary[\"params\"][(\"meta\", \"el\")])\n",
    "\n",
    "from mlens.ensemble import SequentialEnsemble\n",
    "\n",
    "ens = SequentialEnsemble(verbose=5)\n",
    "ens.add('blend',base_learners)\n",
    "ens.add_meta([('XGBoost', XGBoost)])\n",
    "\n",
    "ens.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ens.predict(X_valid)\n",
    "print \"mean_absolute_error: \", metrics.mean_absolute_error(y_valid.values.ravel(), y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = ens.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "print \"Writing ...\"\n",
    "sub = pd.read_csv('Data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "sub.to_csv('ensemble.csv', index=False, float_format='%.4f')\n",
    "\n",
    "print \"Zipping ...\"\n",
    "with zipfile.ZipFile(\"submission.zip\", \"w\") as fout:\n",
    "    fout.write(\"ensemble.csv\", compress_type=zipfile.ZIP_DEFLATED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# summary = pd.DataFrame(evl.summary)\n",
    "# writer = pd.ExcelWriter('output.xlsx')\n",
    "# summary.to_excel(writer,'Sheet1')\n",
    "# writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 集成学习\n",
    "\n",
    "1. 双层集成学习  \n",
    "2. 使用尽可能多的分类器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (90275, 54)\n",
      "y_train.shape: (90275,)\n",
      "X_test.shape: (2985217, 54)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, columns = load_supervision()\n",
    "print 'X_train.shape:', X_train.shape\n",
    "print 'y_train.shape:', y_train.shape\n",
    "print 'X_test.shape:', X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = X_train.values, y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def base_layer(X, y, clf_base_layer):\n",
    "    \n",
    "    train_sec = np.zeros((len(y), len(clf_base_layer)))\n",
    "    \n",
    "    for i, (clf_name, clf) in enumerate(clf_base_layer):\n",
    "        print time.ctime(),  'first layer classifier:', clf_name\n",
    "        # cross validation\n",
    "        y_score = cross_val_predict(clf, X, y, cv=10, verbose=5, method='predict')\n",
    "        train_sec[:, i] = y_score\n",
    "        print 'mean_absolute_error: ', metrics.mean_absolute_error(y, y_score)\n",
    "        \n",
    "    return train_sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Fri Jun 30 11:44:57 2017 first layer classifier: RandomForest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   31.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  5.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error:  0.0824276123966\n",
      "Fri Jun 30 11:50:17 2017 first layer classifier: AdaBoost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   14.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   29.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   44.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   58.8s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  2.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error:  0.154825126029\n",
      "Fri Jun 30 11:52:45 2017 first layer classifier: GBDT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   25.9s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   51.7s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  1.7min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  4.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error:  0.0684507766055\n",
      "Fri Jun 30 11:57:03 2017 first layer classifier: Bagging\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   33.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.1min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.6min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.2min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  5.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error:  0.082436511228\n",
      "Fri Jun 30 12:02:27 2017 first layer classifier: ExtraTrees\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   21.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:   32.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:   43.6s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:  1.8min finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_absolute_error:  0.0845610893909\n",
      "Fri Jun 30 12:04:16 2017 second layer classifier: LogisticRegression\n",
      "mean_absolute_error:  0.0682835779271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "clf_first_layer = [('RandomForest', ensemble.RandomForestRegressor()), \n",
    "                   ('AdaBoost', ensemble.AdaBoostRegressor()), \n",
    "                   ('GBDT', ensemble.GradientBoostingRegressor()), \n",
    "                   ('Bagging', ensemble.BaggingRegressor()), \n",
    "                   ('ExtraTrees', ensemble.ExtraTreesRegressor())\n",
    "                  ]\n",
    "\n",
    "clf_secon_layer = linear_model.LinearRegression()\n",
    "\n",
    "# first layer\n",
    "train_sec = first_layer_feature(X, y, clf_first_layer)\n",
    "\n",
    "# second layer\n",
    "print time.ctime(), 'second layer classifier:', 'LogisticRegression'\n",
    "y_score = cross_val_predict(clf_secon_layer, train_sec, y, cv=10, verbose=5, method='predict')\n",
    "print 'mean_absolute_error: ', metrics.mean_absolute_error(y, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_sec.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf_single = linear_model.ARDRegression()\n",
    "\n",
    "y_score = cross_val_predict(clf_single, X, y, cv=10, verbose=5, method='predict')\n",
    "print 'mean_absolute_error: ', metrics.mean_absolute_error(y, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.04093</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.03999</td>\n",
       "      <td>-0.01541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.01264</td>\n",
       "      <td>0.196108</td>\n",
       "      <td>0.010603</td>\n",
       "      <td>0.00035</td>\n",
       "      <td>-0.00241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.13674</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>-0.007266</td>\n",
       "      <td>-0.01599</td>\n",
       "      <td>-0.02144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.00002</td>\n",
       "      <td>0.049947</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>-0.01011</td>\n",
       "      <td>0.01299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.01015</td>\n",
       "      <td>-0.077274</td>\n",
       "      <td>0.004682</td>\n",
       "      <td>0.00359</td>\n",
       "      <td>-0.00041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2        3        4\n",
       "0 -0.04093  0.049947  0.004242  0.03999 -0.01541\n",
       "1  0.01264  0.196108  0.010603  0.00035 -0.00241\n",
       "2 -0.13674  0.049947 -0.007266 -0.01599 -0.02144\n",
       "3 -0.00002  0.049947  0.006979 -0.01011  0.01299\n",
       "4  0.01015 -0.077274  0.004682  0.00359 -0.00041"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train_sec).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Genetic Programming （遗传算法/进化规划）:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, y_train = X.values, y.values.ravel()\n",
    "\n",
    "standardScaler = StandardScaler()\n",
    "X_train = standardScaler.fit_transform(X_train)\n",
    "X_test = standardScaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Genetic Programming\n",
    "from gplearn.skutils import check_random_state\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# train GP\n",
    "function_set = ['add','sub','mul','div','sqrt','log','abs','neg','inv','max','min','sin','cos','tan']\n",
    "\n",
    "titan_gp = SymbolicRegressor(population_size=2000,\n",
    "                             generations=20, \n",
    "                             stopping_criteria=0.06774,\n",
    "                             p_crossover=0.7, \n",
    "                             p_subtree_mutation=0.1,\n",
    "                             p_hoist_mutation=0.05, \n",
    "                             p_point_mutation=0.1,\n",
    "                             max_samples=0.9, \n",
    "                             verbose=1,\n",
    "                             parsimony_coefficient=0.01,\n",
    "                             function_set=function_set,\n",
    "                             n_jobs=1,\n",
    "                             random_state=2017)\n",
    "titan_gp.fit(X_train, y_train) # np.array([np.random.randint(2) for i in range(len(y_train))])\n",
    "\n",
    "# print relationship\n",
    "print 'Relationship:', titan_gp._program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "y_pred = titan_gp.predict(X_test)\n",
    "\n",
    "sub = pd.read_csv('Data/sample_submission.csv')\n",
    "for c in sub.columns[sub.columns != 'ParcelId']:\n",
    "    sub[c] = y_pred\n",
    "sub.to_csv('gplearn.csv', index=False, float_format='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 标签传播算法\n",
    "\n",
    "1. 标签传播算法只能处理分类问题。这里先将标签去头去尾，再取其小数值构成少量标签。目的是将未标签的值标上标签，增加训练集样本。  \n",
    "2. 不考虑交易日期属性：删除原训练集中的parcelid重复项，与属性集融合，构成一个半监督数据集。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "properties16, train16, sample_submission = load_original()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_test = train16.copy()\n",
    "\n",
    "ulimit = np.percentile(train_test.logerror.values, 99)\n",
    "llimit = np.percentile(train_test.logerror.values, 1)\n",
    "train_test['logerror'].ix[train_test['logerror']>ulimit] = ulimit\n",
    "train_test['logerror'].ix[train_test['logerror']<llimit] = llimit\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.distplot(train_test.logerror.values, kde=True)\n",
    "plt.xlabel('logerror', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "logerror_short = np.array([Decimal(str(i)).quantize(Decimal('0.0')) for i in np.sort(train_test.logerror.values)], dtype=np.float)\n",
    "\n",
    "le = LabelEncoder()\n",
    "logerror_short_label = le.fit_transform(logerror_short)\n",
    "\n",
    "train_test['logerror'] = logerror_short_label\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.distplot(train_test.logerror.values, kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "对属性值做PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def build_proper():\n",
    "    properties = properties16.copy()\n",
    "\n",
    "    properties = properties.drop(['propertyzoningdesc', 'taxdelinquencyflag', 'propertycountylandusecode'], axis=1)\n",
    "\n",
    "    properties.fillna(0, inplace=True)\n",
    "\n",
    "    convert = properties.dtypes == 'object'\n",
    "    properties.loc[:, convert] = properties.loc[:, convert].apply(lambda x: 1 * (x == True))\n",
    "\n",
    "    pca = PCA(n_components=5)\n",
    "    properties_pca = pca.fit_transform(properties)\n",
    "    \n",
    "    properties_pca = pd.DataFrame(properties_pca)\n",
    "    properties_pca['parcelid'] = properties16['parcelid']\n",
    "    \n",
    "    return pd.DataFrame(properties_pca)\n",
    "\n",
    "\n",
    "train = pd.merge(train_test.drop_duplicates('parcelid'), build_proper(), on=\"parcelid\", how=\"outer\")\n",
    "\n",
    "# 将logerror为NaN的值换成-1\n",
    "train['logerror'].fillna(-1, inplace=True)\n",
    "\n",
    "y = train.logerror\n",
    "train = train.drop(['parcelid', 'logerror', 'transactiondate'], axis=1)\n",
    "\n",
    "for c, dtype in zip(train.columns, train.dtypes):\n",
    "    if dtype == np.float64:\n",
    "        train[c] = train[c].astype(np.float32)\n",
    "    if dtype == np.int64:\n",
    "        train[c] = train[c].astype(np.int8)\n",
    "\n",
    "train = train.values\n",
    "y = y.values\n",
    "\n",
    "print train.shape\n",
    "print y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()\n",
    "train_ss = ss.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_ss, y, test_size=0.005, random_state=SEED)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_test, y_test, test_size=0.2, random_state=SEED)\n",
    "\n",
    "print X_train.shape\n",
    "print X_test.shape\n",
    "\n",
    "# skf = StratifiedKFold(y, n_folds=10)\n",
    "# for train_index, test_index in skf:\n",
    "#     X_train, X_test = train_ss[train_index], train_ss[test_index]\n",
    "#     y_train, y_test = y[train_index], y[test_index]\n",
    "#     print(\"=== TRAIN:\", X_train.shape, \"TEST:\", X_test.shape)\n",
    "    \n",
    "#     skf2 = StratifiedKFold(y_test, n_folds=5)\n",
    "#     for train_index2, test_index2 in skf2:\n",
    "#         X_train2, X_test2 = X_test[train_index2], X_test[test_index2]\n",
    "#         y_train2, y_test2 = y_test[train_index2], y_test[test_index2]\n",
    "#         print(\"TRAIN2:\", X_train2.shape, \"TEST2:\", X_test2.shape)\n",
    "#         ls.fit(X_train2, y_train2)\n",
    "#         y_pred2 = ls.predict(X_test2)\n",
    "#         print metrics.classification_report(y_test2, y_pred2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ls = LabelSpreading()\n",
    "\n",
    "ls.fit(X_train, y_train)\n",
    "y_pred = ls.predict(X_test)\n",
    "# print metrics.classification_report(y_test, y_pred)\n",
    "pd.DataFrame({\"True\": y_test, \"Pred\": y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sns.jointplot(y_pred, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
