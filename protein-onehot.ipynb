{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DATE: 2017, 3, 17\n",
    "1. 说明：用one-hot编码，利用MXNet设计卷积神经网络（MLP，LeNET，CIFAR 10 NET），对RNA序列实现二分类、多分类，评价指标为mse，准确率，F1。\n",
    "\n",
    "2. Tricks：\n",
    " * 解决序列长短不一问题。程序先从所有样本中选出最长的序列，以它的长度depth作为共同特征矩阵的长和宽，也就是形成${\\rm{depth}} \\times {\\rm{depth}}$的矩阵，长度不够的位置补全零数组，类似于图片的稀疏像素。若depth为4，序列AUC可编码为：\n",
    "$$ \\left[ {\\begin{array}{*{20}{c}}\n",
    "0&1&0&0\\\\\n",
    "0&0&1&0\\\\\n",
    "0&0&0&1\\\\\n",
    "0&0&0&0\n",
    "\\end{array}} \\right] $$\n",
    "\n",
    " * 但是对于过长的序列会产生过高维度的特征，可以采用香农编码或哈夫曼编码压缩编码解决。\n",
    "\n",
    "DATE: 2017, 3, 24\n",
    "1. 用哈夫曼编码暂时不考虑，使用最简单的A=1，,U=2，C=3，G=4来编码，每条样本形成${\\rm{100}} \\times {\\rm{100}}$的矩阵，该种效果与直接使用pse-in-one提取出的特征分类相似；\n",
    "2. 考虑使用4个字母的连续256种组合编码，与图像的3通道像素相似。暂时无实验价值；\n",
    "3. 考虑直接将卷积出的抽象特征输出，连接SVM做分类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 20:31:53.501981. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 设计卷积网络\n",
    "# coding=utf-8\n",
    "\n",
    "import time\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "\n",
    "# Basic Conv + BN + ReLU factory\n",
    "def ConvFactory(data, num_filter, kernel, stride=(1, 1), pad=(0, 0), act_type=\"relu\"):\n",
    "    # there is an optional parameter ```wrokshpace``` may influece convolution performance\n",
    "    # default, the workspace is set to 256(MB)\n",
    "    # you may set larger value, but convolution layer only requires its needed but not exactly\n",
    "    # MXNet will handle reuse of workspace without parallelism conflict\n",
    "    conv = mx.symbol.Convolution(data=data, workspace=256,\n",
    "                                 num_filter=num_filter, kernel=kernel, stride=stride, pad=pad)\n",
    "    bn = mx.symbol.BatchNorm(data=conv)\n",
    "    act = mx.symbol.Activation(data=bn, act_type=act_type)\n",
    "    return act\n",
    "\n",
    "\n",
    "# A Simple Downsampling Factory\n",
    "def DownsampleFactory(data, ch_3x3):\n",
    "    # conv 3x3\n",
    "    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1))\n",
    "    # pool\n",
    "    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max')\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv, pool])\n",
    "    return concat\n",
    "\n",
    "\n",
    "# A Simple module\n",
    "def SimpleFactory(data, ch_1x1, ch_3x3):\n",
    "    # 1x1\n",
    "    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1)\n",
    "    # 3x3\n",
    "    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3)\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv1x1, conv3x3])\n",
    "    return concat\n",
    "\n",
    "\n",
    "def set_cifar10():\n",
    "    data = mx.symbol.Variable(name=\"data\")\n",
    "    conv1 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=96, act_type=\"relu\")\n",
    "    in3a = SimpleFactory(conv1, 32, 32)\n",
    "    in3b = SimpleFactory(in3a, 32, 48)\n",
    "    in3c = DownsampleFactory(in3b, 80)\n",
    "    in4a = SimpleFactory(in3c, 112, 48)\n",
    "    in4b = SimpleFactory(in4a, 96, 64)\n",
    "    in4c = SimpleFactory(in4b, 80, 80)\n",
    "    in4d = SimpleFactory(in4c, 48, 96)\n",
    "    in4e = DownsampleFactory(in4d, 96)\n",
    "    in5a = SimpleFactory(in4e, 176, 160)\n",
    "    in5b = SimpleFactory(in5a, 176, 160)\n",
    "    pool = mx.symbol.Pooling(data=in5b, pool_type=\"avg\", kernel=(7, 7), name=\"global_avg\")\n",
    "    flatten = mx.symbol.Flatten(data=pool)\n",
    "    fc = mx.symbol.FullyConnected(data=flatten, num_hidden=10)\n",
    "    softmax = mx.symbol.SoftmaxOutput(name='softmax', data=fc)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "# 卷积神经网络\n",
    "def set_con():\n",
    "    data = mx.symbol.Variable('data')\n",
    "    # first conv layer\n",
    "    mx.sym.Activation\n",
    "    conv1 = mx.sym.Convolution(data=data, kernel=(5, 5), num_filter=20)\n",
    "    tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # second conv layer\n",
    "    conv2 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\n",
    "    tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # first fullc layer\n",
    "    flatten = mx.sym.Flatten(data=pool2)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "    tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "    # second fullc\n",
    "    fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=42)\n",
    "    # softmax loss\n",
    "    lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax') \n",
    "    return lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> loading datasets ...  finished. 15.593175 ms\n",
      ">> encoding ...  finished. 24.803945 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 21:03:40.597723. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 载入数据，并进行one-hot编码\n",
    "\n",
    "import random\n",
    "\n",
    "def load_data(encode='common'):\n",
    "    data_list = []\n",
    "    start = time.clock()\n",
    "    print '>> loading datasets ... ',\n",
    "    for line in open('/home01/shixiangwan/deep_learning/protein_location/RNA-all-long-CD-HIT.fasta'):\n",
    "        if line[0] != '>':\n",
    "            line = line.strip().replace('A', '1').replace('U', '2').replace('C', '3').replace('G', '4')\n",
    "            data_list.append(map(int, list(line)))\n",
    "    depth = len(max(data_list, key=len))\n",
    "    print 'finished.', time.clock() - start, 'ms'\n",
    "\n",
    "    print '>> encoding ... ',\n",
    "    train_set = []\n",
    "    if encode == 'onehot':\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_mx = mx.nd.one_hot(mx.nd.array(data_list[elem], dtype=np.int32), depth=depth) \\\n",
    "                .asnumpy().reshape(len(data_list[elem]) * depth)\n",
    "            tmp_mx = np.append(tmp_mx, np.zeros(len(data_list[elem]) * depth))\n",
    "            train_set.append(list(tmp_mx))\n",
    "    else:\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_list = data_list[elem]\n",
    "            tmp_mx = np.append(tmp_list, np.zeros(10000 - len(tmp_list)))\n",
    "            train_set.append(list(tmp_mx))\n",
    "\n",
    "    label_set = []\n",
    "    for line in open(\"labels_1.txt\"):\n",
    "#         label_set.append(map(float, line.strip().split(',')))   # multi-label\n",
    "        label_set.append(float(line.strip()))                   # single-label\n",
    "    print 'finished.', time.clock() - start, 'ms'\n",
    "    random.seed(100)\n",
    "    random.shuffle(train_set)\n",
    "    random.seed(100)\n",
    "    random.shuffle(label_set)\n",
    "    return train_set, label_set\n",
    "\n",
    "train_set, label_set = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 21:04:12.805861. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 将数据变成类似图像的矩阵形式\n",
    "\n",
    "def to4d(data):\n",
    "    print 'data.shape:', data.shape\n",
    "    return data.reshape(data.shape[0], 1, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (11591, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "INFO:root:Start training with [gpu(1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (1449, 10000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [200]\tSpeed: 17569.56 samples/sec\tTrain-accuracy=0.938214\n",
      "INFO:root:Epoch[0] Batch [400]\tSpeed: 18857.84 samples/sec\tTrain-accuracy=0.942381\n",
      "INFO:root:Epoch[0] Batch [600]\tSpeed: 18664.36 samples/sec\tTrain-accuracy=0.937857\n",
      "INFO:root:Epoch[0] Batch [800]\tSpeed: 18758.87 samples/sec\tTrain-accuracy=0.940000\n",
      "INFO:root:Epoch[0] Batch [1000]\tSpeed: 18666.19 samples/sec\tTrain-accuracy=0.941310\n",
      "INFO:root:Epoch[0] Batch [1200]\tSpeed: 18916.86 samples/sec\tTrain-accuracy=0.940714\n",
      "INFO:root:Epoch[0] Batch [1400]\tSpeed: 18593.39 samples/sec\tTrain-accuracy=0.939643\n",
      "INFO:root:Epoch[0] Batch [1600]\tSpeed: 18838.30 samples/sec\tTrain-accuracy=0.942619\n",
      "INFO:root:Epoch[0] Batch [1800]\tSpeed: 19007.90 samples/sec\tTrain-accuracy=0.942738\n",
      "INFO:root:Epoch[0] Batch [2000]\tSpeed: 18820.30 samples/sec\tTrain-accuracy=0.945833\n",
      "INFO:root:Epoch[0] Batch [2200]\tSpeed: 18596.85 samples/sec\tTrain-accuracy=0.939762\n",
      "INFO:root:Epoch[0] Batch [2400]\tSpeed: 18561.00 samples/sec\tTrain-accuracy=0.940119\n",
      "INFO:root:Epoch[0] Batch [2600]\tSpeed: 18781.99 samples/sec\tTrain-accuracy=0.941310\n",
      "INFO:root:Epoch[0] Batch [2800]\tSpeed: 18827.34 samples/sec\tTrain-accuracy=0.941429\n",
      "INFO:root:Epoch[0] Batch [3000]\tSpeed: 18810.53 samples/sec\tTrain-accuracy=0.942381\n",
      "INFO:root:Epoch[0] Batch [3200]\tSpeed: 18798.67 samples/sec\tTrain-accuracy=0.939524\n",
      "INFO:root:Epoch[0] Batch [3400]\tSpeed: 18795.74 samples/sec\tTrain-accuracy=0.937619\n",
      "INFO:root:Epoch[0] Batch [3600]\tSpeed: 18628.69 samples/sec\tTrain-accuracy=0.938095\n",
      "INFO:root:Epoch[0] Batch [3800]\tSpeed: 18604.11 samples/sec\tTrain-accuracy=0.940119\n",
      "INFO:root:Epoch[0] Batch [4000]\tSpeed: 18758.12 samples/sec\tTrain-accuracy=0.939762\n",
      "INFO:root:Epoch[0] Batch [4200]\tSpeed: 18740.21 samples/sec\tTrain-accuracy=0.940476\n",
      "INFO:root:Epoch[0] Batch [4400]\tSpeed: 18804.95 samples/sec\tTrain-accuracy=0.941548\n",
      "INFO:root:Epoch[0] Batch [4600]\tSpeed: 18813.21 samples/sec\tTrain-accuracy=0.942976\n",
      "INFO:root:Epoch[0] Batch [4800]\tSpeed: 18746.41 samples/sec\tTrain-accuracy=0.941310\n",
      "INFO:root:Epoch[0] Batch [5000]\tSpeed: 18633.20 samples/sec\tTrain-accuracy=0.942143\n",
      "INFO:root:Epoch[0] Batch [5200]\tSpeed: 18533.16 samples/sec\tTrain-accuracy=0.944286\n",
      "INFO:root:Epoch[0] Batch [5400]\tSpeed: 18805.90 samples/sec\tTrain-accuracy=0.938452\n",
      "INFO:root:Epoch[0] Batch [5600]\tSpeed: 18667.95 samples/sec\tTrain-accuracy=0.941429\n",
      "INFO:root:Epoch[0] Batch [5800]\tSpeed: 18729.15 samples/sec\tTrain-accuracy=0.941786\n",
      "INFO:root:Epoch[0] Batch [6000]\tSpeed: 18537.88 samples/sec\tTrain-accuracy=0.941190\n",
      "INFO:root:Epoch[0] Batch [6200]\tSpeed: 18774.39 samples/sec\tTrain-accuracy=0.942619\n",
      "INFO:root:Epoch[0] Batch [6400]\tSpeed: 19014.12 samples/sec\tTrain-accuracy=0.940119\n",
      "INFO:root:Epoch[0] Batch [6600]\tSpeed: 18850.83 samples/sec\tTrain-accuracy=0.942024\n",
      "INFO:root:Epoch[0] Batch [6800]\tSpeed: 18822.08 samples/sec\tTrain-accuracy=0.947738\n",
      "INFO:root:Epoch[0] Batch [7000]\tSpeed: 18800.44 samples/sec\tTrain-accuracy=0.937143\n",
      "INFO:root:Epoch[0] Batch [7200]\tSpeed: 18909.69 samples/sec\tTrain-accuracy=0.940714\n",
      "INFO:root:Epoch[0] Batch [7400]\tSpeed: 18967.72 samples/sec\tTrain-accuracy=0.941667\n",
      "INFO:root:Epoch[0] Batch [7600]\tSpeed: 18875.28 samples/sec\tTrain-accuracy=0.939643\n",
      "INFO:root:Epoch[0] Batch [7800]\tSpeed: 18785.09 samples/sec\tTrain-accuracy=0.945714\n",
      "INFO:root:Epoch[0] Batch [8000]\tSpeed: 18852.58 samples/sec\tTrain-accuracy=0.942619\n",
      "INFO:root:Epoch[0] Batch [8200]\tSpeed: 18774.81 samples/sec\tTrain-accuracy=0.940595\n",
      "INFO:root:Epoch[0] Batch [8400]\tSpeed: 18925.40 samples/sec\tTrain-accuracy=0.943333\n",
      "INFO:root:Epoch[0] Batch [8600]\tSpeed: 18821.21 samples/sec\tTrain-accuracy=0.940833\n",
      "INFO:root:Epoch[0] Batch [8800]\tSpeed: 18811.42 samples/sec\tTrain-accuracy=0.941429\n",
      "INFO:root:Epoch[0] Batch [9000]\tSpeed: 19012.44 samples/sec\tTrain-accuracy=0.941905\n",
      "INFO:root:Epoch[0] Batch [9200]\tSpeed: 18834.59 samples/sec\tTrain-accuracy=0.937500\n",
      "INFO:root:Epoch[0] Batch [9400]\tSpeed: 18843.52 samples/sec\tTrain-accuracy=0.938452\n",
      "INFO:root:Epoch[0] Batch [9600]\tSpeed: 18532.43 samples/sec\tTrain-accuracy=0.942024\n",
      "INFO:root:Epoch[0] Batch [9800]\tSpeed: 18443.00 samples/sec\tTrain-accuracy=0.940952\n",
      "INFO:root:Epoch[0] Batch [10000]\tSpeed: 18913.41 samples/sec\tTrain-accuracy=0.943690\n",
      "INFO:root:Epoch[0] Batch [10200]\tSpeed: 18623.62 samples/sec\tTrain-accuracy=0.944405\n",
      "INFO:root:Epoch[0] Batch [10400]\tSpeed: 18713.62 samples/sec\tTrain-accuracy=0.940952\n",
      "INFO:root:Epoch[0] Batch [10600]\tSpeed: 18331.25 samples/sec\tTrain-accuracy=0.940238\n",
      "INFO:root:Epoch[0] Batch [10800]\tSpeed: 18953.16 samples/sec\tTrain-accuracy=0.942976\n",
      "INFO:root:Epoch[0] Batch [11000]\tSpeed: 18796.90 samples/sec\tTrain-accuracy=0.939881\n",
      "INFO:root:Epoch[0] Batch [11200]\tSpeed: 18440.10 samples/sec\tTrain-accuracy=0.938929\n",
      "INFO:root:Epoch[0] Batch [11400]\tSpeed: 18787.35 samples/sec\tTrain-accuracy=0.940833\n",
      "INFO:root:Epoch[0] Resetting Data Iterator\n",
      "INFO:root:Epoch[0] Time cost=62.074\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.947846\n",
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 20:32:02.388933. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 对已经载入的数据进行深度学习\n",
    "\n",
    "all_number = len(label_set)\n",
    "# print all_number\n",
    "# print np.array(train_set).shape\n",
    "split_tv = int(0.8 * all_number)\n",
    "split_tt = int(0.9 * all_number)\n",
    "train_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[0:split_tv]))),\n",
    "                               mx.nd.array(np.array(label_set[0:split_tv])),\n",
    "                               shuffle=True)\n",
    "validate_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[split_tv:split_tt]))),\n",
    "                                  mx.nd.array(np.array(label_set[split_tv:split_tt])),\n",
    "                                  shuffle=True)\n",
    "\n",
    "# train\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=set_con(),  # set_mlp(), set_con(), set_cifar10()\n",
    "                             num_epoch=1,\n",
    "                             learning_rate=0.1,\n",
    "                             momentum=0.9,\n",
    "                             wd=0.00001)\n",
    "\n",
    "batch_size = 100\n",
    "result = model.fit(X=train_iter,\n",
    "                   eval_data=validate_iter,\n",
    "                   batch_end_callback=mx.callback.Speedometer(batch_size, 200)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'convolution20_weight',\n",
       " 'convolution20_bias',\n",
       " 'convolution20_output',\n",
       " 'activation30_output',\n",
       " 'pooling20_output',\n",
       " 'convolution21_weight',\n",
       " 'convolution21_bias',\n",
       " 'convolution21_output',\n",
       " 'activation31_output',\n",
       " 'pooling21_output',\n",
       " 'flatten10_output',\n",
       " 'fullyconnected20_weight',\n",
       " 'fullyconnected20_bias',\n",
       " 'fullyconnected20_output',\n",
       " 'activation32_output',\n",
       " 'fullyconnected21_weight',\n",
       " 'fullyconnected21_bias',\n",
       " 'fullyconnected21_output',\n",
       " 'softmax_label',\n",
       " 'softmax_output']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 21:05:00.415786. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 列出模型中的所有层\n",
    "internals = model.symbol.get_internals()\n",
    "internals.list_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:8: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> results' number 14489\n",
      ">> test_lbl' number 14489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 21:09:58.396821. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 确定新层\n",
    "fea_symbol = internals['fullyconnected20_output']\n",
    "new_model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=fea_symbol,\n",
    "                             numpy_batch_size=1,\n",
    "                             arg_params=model.arg_params,\n",
    "                             aux_params=model.aux_params,\n",
    "                             allow_extra_params=True)\n",
    "\n",
    "# 提取结果到arff文件\n",
    "test_set = train_set      # train_set[split_tt:], train_set\n",
    "test_lbl = label_set      # label_set[split_tt:], label_set\n",
    "\n",
    "results = []\n",
    "for i in range(len(test_set)):\n",
    "    test_val = np.array(test_set[i]).reshape(1, 1, 100, 100)\n",
    "    prob = new_model.predict(test_val)\n",
    "    # print len(prob[0])\n",
    "    results.append(prob[0])\n",
    "\n",
    "print \">> results' number\", len(results)\n",
    "print \">> test_lbl' number\", len(test_lbl)\n",
    "arff_results = open('lenet.arff', 'w')\n",
    "arff_results.write('@relation lenet\\n')\n",
    "for i in range(len(results[0])):\n",
    "    arff_results.write('@attribute Feature' + str(i+1) + ' real\\n')\n",
    "# for i in range(len(test_lbl[0])):\n",
    "#     arff_results.write('@attribute class' + str(i+1) + ' {0.0,1.0}\\n')\n",
    "arff_results.write('@attribute class {0.0,1.0}\\n')\n",
    "arff_results.write('@data\\n')\n",
    "for i in range(len(results)):\n",
    "    # line =  ','.join(str(j) for j in results[i]) + ',' + ','.join(str(j) for j in test_lbl[i])\n",
    "    line =  ','.join(str(j) for j in results[i]) + ',' + str(test_lbl[i])\n",
    "    arff_results.write(line + '\\n')\n",
    "arff_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 正例数（标签1.0）： 13271\n",
      ">> 反例数（标签0.0）： 1218\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-28 21:45:38.062258. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "#  随机分割不平衡数据集，生成arff文件\n",
    "import random\n",
    "\n",
    "def split_unbalance(file_name, pos_lab, neg_lab):\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    suffix = ''\n",
    "    for line in open(file_name):\n",
    "        if line[0] == '@':\n",
    "            suffix += line\n",
    "        else:\n",
    "            if line.strip().split(',')[-1] == pos_lab:\n",
    "                pos_list.append(line.strip())\n",
    "            elif line.strip().split(',')[-1] == neg_lab:\n",
    "                neg_list.append(line.strip())\n",
    "    pos_len = len(pos_list)\n",
    "    neg_len = len(neg_list)\n",
    "    print '>> 正例数（标签1.0）：', pos_len\n",
    "    print '>> 反例数（标签0.0）：', neg_len\n",
    "    if pos_len > neg_len:\n",
    "        new_list = random.sample(pos_list, neg_len)\n",
    "        return suffix, new_list, neg_list\n",
    "    elif pos_len < neg_len:\n",
    "        new_list = random.sample(neg_list, pos_len)\n",
    "        return suffix, pos_list, new_list\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "input_file = \"lenet.arff\"\n",
    "output_file = \"lenet-b.arff\"\n",
    "pos_lab = \"1.0\"\n",
    "neg_lab = \"0.0\"\n",
    "suffix, one_list, two_list = split_unbalance(input_file, pos_lab, neg_lab)\n",
    "\n",
    "if None is suffix:\n",
    "    print 'Blanced Dataset !'\n",
    "    exit()\n",
    "\n",
    "one_list.extend(two_list)\n",
    "random.shuffle(one_list)\n",
    "results = open(output_file, \"w\")\n",
    "results.write(suffix)\n",
    "for i in range(len(one_list)):\n",
    "    results.write(one_list[i] + '\\n')\n",
    "results.close()\n",
    "\n",
    "print 'Finished.'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
