{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DATE: 2017, 3, 17\n",
    "1. 说明：用one-hot编码，利用MXNet设计卷积神经网络（MLP，LeNET，CIFAR 10 NET），对RNA序列实现二分类、多分类，评价指标为mse，准确率，F1。\n",
    "\n",
    "2. Tricks：\n",
    " * 解决序列长短不一问题。程序先从所有样本中选出最长的序列，以它的长度depth作为共同特征矩阵的长和宽，也就是形成${\\rm{depth}} \\times {\\rm{depth}}$的矩阵，长度不够的位置补全零数组，类似于图片的稀疏像素。若depth为4，序列AUC可编码为：\n",
    "$$ \\left[ {\\begin{array}{*{20}{c}}\n",
    "0&1&0&0\\\\\n",
    "0&0&1&0\\\\\n",
    "0&0&0&1\\\\\n",
    "0&0&0&0\n",
    "\\end{array}} \\right] $$\n",
    "\n",
    " * 但是对于过长的序列会产生过高维度的特征，可以采用香农编码或哈夫曼编码压缩编码解决。\n",
    "\n",
    "DATE: 2017, 3, 24\n",
    "1. 用哈夫曼编码暂时不考虑，使用最简单的A=1，,U=2，C=3，G=4来编码，每条样本形成${\\rm{100}} \\times {\\rm{100}}$的矩阵，该种效果与直接使用pse-in-one提取出的特征分类相似；\n",
    "2. 考虑使用4个字母的连续256种组合编码，与图像的3通道像素相似。暂时无实验价值；\n",
    "3. 考虑直接将卷积出的抽象特征输出，连接SVM做分类。\n",
    "\n",
    "DATE: 2017, 3, 29\n",
    "1. 直接将卷积出的抽象特征输出，连接sklearn或weka做分类，效果都还是不好，为什么分类准确度这么低？50%？\n",
    "2. 有没有其他可行的方案？思考中......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:06:17.627466. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 设计卷积网络\n",
    "# coding=utf-8\n",
    "\n",
    "import time\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "    \n",
    "# Basic Conv + BN + ReLU factory\n",
    "def ConvFactory(data, num_filter, kernel, stride=(1, 1), pad=(0, 0), act_type=\"relu\"):\n",
    "    # there is an optional parameter ```wrokshpace``` may influece convolution performance\n",
    "    # default, the workspace is set to 256(MB)\n",
    "    # you may set larger value, but convolution layer only requires its needed but not exactly\n",
    "    # MXNet will handle reuse of workspace without parallelism conflict\n",
    "    conv = mx.symbol.Convolution(data=data, workspace=256,\n",
    "                                 num_filter=num_filter, kernel=kernel, stride=stride, pad=pad)\n",
    "    bn = mx.symbol.BatchNorm(data=conv)\n",
    "    act = mx.symbol.Activation(data=bn, act_type=act_type)\n",
    "    return act\n",
    "\n",
    "\n",
    "# A Simple Downsampling Factory\n",
    "def DownsampleFactory(data, ch_3x3):\n",
    "    # conv 3x3\n",
    "    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1))\n",
    "    # pool\n",
    "    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max')\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv, pool])\n",
    "    return concat\n",
    "\n",
    "\n",
    "# A Simple module\n",
    "def SimpleFactory(data, ch_1x1, ch_3x3):\n",
    "    # 1x1\n",
    "    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1)\n",
    "    # 3x3\n",
    "    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3)\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv1x1, conv3x3])\n",
    "    return concat\n",
    "\n",
    "\n",
    "def set_cifar10():\n",
    "    data = mx.symbol.Variable(name=\"data\")\n",
    "    conv1 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=96, act_type=\"relu\")\n",
    "    in3a = SimpleFactory(conv1, 32, 32)\n",
    "    in3b = SimpleFactory(in3a, 32, 48)\n",
    "    in3c = DownsampleFactory(in3b, 80)\n",
    "    in4a = SimpleFactory(in3c, 112, 48)\n",
    "    in4b = SimpleFactory(in4a, 96, 64)\n",
    "    in4c = SimpleFactory(in4b, 80, 80)\n",
    "    in4d = SimpleFactory(in4c, 48, 96)\n",
    "    in4e = DownsampleFactory(in4d, 96)\n",
    "    in5a = SimpleFactory(in4e, 176, 160)\n",
    "    in5b = SimpleFactory(in5a, 176, 160)\n",
    "    pool = mx.symbol.Pooling(data=in5b, pool_type=\"avg\", kernel=(7, 7), name=\"global_avg\")\n",
    "    flatten = mx.symbol.Flatten(data=pool)\n",
    "    fc = mx.symbol.FullyConnected(data=flatten, num_hidden=10)\n",
    "    softmax = mx.symbol.SoftmaxOutput(name='softmax', data=fc)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "# 卷积神经网络\n",
    "def set_con():\n",
    "    data = mx.symbol.Variable('data')\n",
    "    # first conv layer\n",
    "    mx.sym.Activation\n",
    "    conv1 = mx.sym.Convolution(data=data, kernel=(5, 5), num_filter=20)\n",
    "    tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # second conv layer\n",
    "    conv2 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\n",
    "    tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # first fullc layer\n",
    "    flatten = mx.sym.Flatten(data=pool2)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "    tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "    # second fullc\n",
    "    fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=42)\n",
    "    # softmax loss\n",
    "    lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax') \n",
    "    return lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> loading datasets ...  finished. 15.645959 s\n",
      ">> encoding ...  finished. 24.526846 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:06:21.010416. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 载入数据，并进行one-hot编码\n",
    "\n",
    "import random\n",
    "\n",
    "def load_data(encode='common'):\n",
    "    data_list = []\n",
    "    start = time.clock()\n",
    "    print '>> loading datasets ... ',\n",
    "    for line in open('/home01/shixiangwan/deep_learning/protein_location/RNA-all-long-CD-HIT.fasta'):\n",
    "        if line[0] != '>':\n",
    "            line = line.strip().replace('A', '1').replace('U', '2').replace('C', '3').replace('G', '4')\n",
    "            data_list.append(map(int, list(line)))\n",
    "    depth = len(max(data_list, key=len))\n",
    "    print 'finished.', time.clock() - start, 's'\n",
    "\n",
    "    print '>> encoding ... ',\n",
    "    train_set = []\n",
    "    if encode == 'onehot':\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_mx = mx.nd.one_hot(mx.nd.array(data_list[elem], dtype=np.int32), depth=depth) \\\n",
    "                .asnumpy().reshape(len(data_list[elem]) * depth)\n",
    "            tmp_mx = np.append(tmp_mx, np.zeros(len(data_list[elem]) * depth))\n",
    "            train_set.append(list(tmp_mx))\n",
    "    else:\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_list = data_list[elem]\n",
    "            tmp_mx = np.append(tmp_list, np.zeros(10000 - len(tmp_list)))\n",
    "            train_set.append(list(tmp_mx))\n",
    "\n",
    "    label_set = []\n",
    "    for line in open(\"labels_1.txt\"):\n",
    "#         label_set.append(map(float, line.strip().split(',')))   # multi-label\n",
    "        label_set.append(float(line.strip()))                   # single-label\n",
    "    print 'finished.', time.clock() - start, 's'\n",
    "    random.seed(100)\n",
    "    random.shuffle(train_set)\n",
    "    random.seed(100)\n",
    "    random.shuffle(label_set)\n",
    "    return train_set, label_set\n",
    "\n",
    "train_set, label_set = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:06:49.834445. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 将数据变成类似图像的矩阵形式\n",
    "\n",
    "def to4d(data):\n",
    "    print 'data.shape:', data.shape\n",
    "    return data.reshape(data.shape[0], 1, 100, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (11591, 10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "/usr/local/lib/python2.7/dist-packages/mxnet-0.9.4-py2.7.egg/mxnet/model.py:516: DeprecationWarning: \u001b[91mCalling initializer with init(str, NDArray) has been deprecated.please use init(mx.init.InitDesc(...), NDArray) instead.\u001b[0m\n",
      "  self.initializer(k, v)\n",
      "INFO:root:Start training with [gpu(1)]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (1449, 10000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:root:Epoch[0] Batch [200]\tSpeed: 11787.80 samples/sec\tTrain-accuracy=0.855000\n",
      "INFO:root:Epoch[0] Batch [400]\tSpeed: 18317.40 samples/sec\tTrain-accuracy=0.825000\n",
      "INFO:root:Epoch[0] Batch [600]\tSpeed: 18395.29 samples/sec\tTrain-accuracy=0.840000\n",
      "INFO:root:Epoch[0] Batch [800]\tSpeed: 18443.72 samples/sec\tTrain-accuracy=0.860000\n",
      "INFO:root:Epoch[0] Batch [1000]\tSpeed: 18172.20 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [1200]\tSpeed: 7873.39 samples/sec\tTrain-accuracy=0.860000\n",
      "INFO:root:Epoch[0] Batch [1400]\tSpeed: 18312.54 samples/sec\tTrain-accuracy=0.870000\n",
      "INFO:root:Epoch[0] Batch [1600]\tSpeed: 18473.74 samples/sec\tTrain-accuracy=0.840000\n",
      "INFO:root:Epoch[0] Batch [1800]\tSpeed: 18752.75 samples/sec\tTrain-accuracy=0.850000\n",
      "INFO:root:Epoch[0] Batch [2000]\tSpeed: 18539.31 samples/sec\tTrain-accuracy=0.825000\n",
      "INFO:root:Epoch[0] Batch [2200]\tSpeed: 18381.64 samples/sec\tTrain-accuracy=0.895000\n",
      "INFO:root:Epoch[0] Batch [2400]\tSpeed: 18567.14 samples/sec\tTrain-accuracy=0.890000\n",
      "INFO:root:Epoch[0] Batch [2600]\tSpeed: 18558.09 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [2800]\tSpeed: 18542.50 samples/sec\tTrain-accuracy=0.900000\n",
      "INFO:root:Epoch[0] Batch [3000]\tSpeed: 18959.16 samples/sec\tTrain-accuracy=0.840000\n",
      "INFO:root:Epoch[0] Batch [3200]\tSpeed: 18434.35 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [3400]\tSpeed: 18481.28 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [3600]\tSpeed: 18687.92 samples/sec\tTrain-accuracy=0.895000\n",
      "INFO:root:Epoch[0] Batch [3800]\tSpeed: 18528.28 samples/sec\tTrain-accuracy=0.885000\n",
      "INFO:root:Epoch[0] Batch [4000]\tSpeed: 18781.39 samples/sec\tTrain-accuracy=0.830000\n",
      "INFO:root:Epoch[0] Batch [4200]\tSpeed: 18714.90 samples/sec\tTrain-accuracy=0.870000\n",
      "INFO:root:Epoch[0] Batch [4400]\tSpeed: 18445.76 samples/sec\tTrain-accuracy=0.910000\n",
      "INFO:root:Epoch[0] Batch [4600]\tSpeed: 18419.09 samples/sec\tTrain-accuracy=0.895000\n",
      "INFO:root:Epoch[0] Batch [4800]\tSpeed: 18781.21 samples/sec\tTrain-accuracy=0.875000\n",
      "INFO:root:Epoch[0] Batch [5000]\tSpeed: 18804.98 samples/sec\tTrain-accuracy=0.810000\n",
      "INFO:root:Epoch[0] Batch [5200]\tSpeed: 18702.16 samples/sec\tTrain-accuracy=0.885000\n",
      "INFO:root:Epoch[0] Batch [5400]\tSpeed: 18483.67 samples/sec\tTrain-accuracy=0.775000\n",
      "INFO:root:Epoch[0] Batch [5600]\tSpeed: 18451.89 samples/sec\tTrain-accuracy=0.825000\n",
      "INFO:root:Epoch[0] Batch [5800]\tSpeed: 18545.72 samples/sec\tTrain-accuracy=0.895000\n",
      "INFO:root:Epoch[0] Batch [6000]\tSpeed: 18592.40 samples/sec\tTrain-accuracy=0.900000\n",
      "INFO:root:Epoch[0] Batch [6200]\tSpeed: 18537.77 samples/sec\tTrain-accuracy=0.800000\n",
      "INFO:root:Epoch[0] Batch [6400]\tSpeed: 18669.87 samples/sec\tTrain-accuracy=0.900000\n",
      "INFO:root:Epoch[0] Batch [6600]\tSpeed: 18577.90 samples/sec\tTrain-accuracy=0.790000\n",
      "INFO:root:Epoch[0] Batch [6800]\tSpeed: 18771.45 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [7000]\tSpeed: 18469.34 samples/sec\tTrain-accuracy=0.770000\n",
      "INFO:root:Epoch[0] Batch [7200]\tSpeed: 18658.48 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [7400]\tSpeed: 18809.15 samples/sec\tTrain-accuracy=0.860000\n",
      "INFO:root:Epoch[0] Batch [7600]\tSpeed: 18714.66 samples/sec\tTrain-accuracy=0.855000\n",
      "INFO:root:Epoch[0] Batch [7800]\tSpeed: 18642.02 samples/sec\tTrain-accuracy=0.775000\n",
      "INFO:root:Epoch[0] Batch [8000]\tSpeed: 18562.78 samples/sec\tTrain-accuracy=0.850000\n",
      "INFO:root:Epoch[0] Batch [8200]\tSpeed: 18703.56 samples/sec\tTrain-accuracy=0.885000\n",
      "INFO:root:Epoch[0] Batch [8400]\tSpeed: 18451.39 samples/sec\tTrain-accuracy=0.785000\n",
      "INFO:root:Epoch[0] Batch [8600]\tSpeed: 18736.33 samples/sec\tTrain-accuracy=0.840000\n",
      "INFO:root:Epoch[0] Batch [8800]\tSpeed: 18623.58 samples/sec\tTrain-accuracy=0.815000\n",
      "INFO:root:Epoch[0] Batch [9000]\tSpeed: 18621.80 samples/sec\tTrain-accuracy=0.870000\n",
      "INFO:root:Epoch[0] Batch [9200]\tSpeed: 18469.58 samples/sec\tTrain-accuracy=0.820000\n",
      "INFO:root:Epoch[0] Batch [9400]\tSpeed: 18569.81 samples/sec\tTrain-accuracy=0.875000\n",
      "INFO:root:Epoch[0] Batch [9600]\tSpeed: 18935.63 samples/sec\tTrain-accuracy=0.850000\n",
      "INFO:root:Epoch[0] Batch [9800]\tSpeed: 18673.87 samples/sec\tTrain-accuracy=0.855000\n",
      "INFO:root:Epoch[0] Batch [10000]\tSpeed: 18638.31 samples/sec\tTrain-accuracy=0.860000\n",
      "INFO:root:Epoch[0] Batch [10200]\tSpeed: 18518.65 samples/sec\tTrain-accuracy=0.820000\n",
      "INFO:root:Epoch[0] Batch [10400]\tSpeed: 18691.24 samples/sec\tTrain-accuracy=0.900000\n",
      "INFO:root:Epoch[0] Batch [10600]\tSpeed: 18343.99 samples/sec\tTrain-accuracy=0.840000\n",
      "INFO:root:Epoch[0] Batch [10800]\tSpeed: 18847.04 samples/sec\tTrain-accuracy=0.870000\n",
      "INFO:root:Epoch[0] Batch [11000]\tSpeed: 18457.64 samples/sec\tTrain-accuracy=0.880000\n",
      "INFO:root:Epoch[0] Batch [11200]\tSpeed: 18680.56 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [11400]\tSpeed: 18595.45 samples/sec\tTrain-accuracy=0.815000\n",
      "INFO:root:Epoch[0] Resetting Data Iterator\n",
      "INFO:root:Epoch[0] Time cost=64.573\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.908213\n",
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:06:53.674091. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 对已经载入的数据进行深度学习\n",
    "\n",
    "all_number = len(label_set)\n",
    "# print all_number\n",
    "# print np.array(train_set).shape\n",
    "split_tv = int(0.8 * all_number)\n",
    "split_tt = int(0.9 * all_number)\n",
    "train_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[0:split_tv]))),\n",
    "                               mx.nd.array(np.array(label_set[0:split_tv])),\n",
    "                               shuffle=True)\n",
    "validate_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[split_tv:split_tt]))),\n",
    "                                  mx.nd.array(np.array(label_set[split_tv:split_tt])),\n",
    "                                  shuffle=True)\n",
    "\n",
    "# train\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=set_con(),  # set_mlp(), set_con(), set_cifar10()\n",
    "                             num_epoch=1,\n",
    "                             learning_rate=0.1,\n",
    "                             momentum=0.9,\n",
    "                             wd=0.00001)\n",
    "\n",
    "batch_size = 100\n",
    "result = model.fit(X=train_iter,\n",
    "                   eval_data=validate_iter,\n",
    "                   batch_end_callback=mx.callback.Speedometer(batch_size, 200)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'convolution0_weight',\n",
       " 'convolution0_bias',\n",
       " 'convolution0_output',\n",
       " 'activation0_output',\n",
       " 'pooling0_output',\n",
       " 'convolution1_weight',\n",
       " 'convolution1_bias',\n",
       " 'convolution1_output',\n",
       " 'activation1_output',\n",
       " 'pooling1_output',\n",
       " 'flatten0_output',\n",
       " 'fullyconnected0_weight',\n",
       " 'fullyconnected0_bias',\n",
       " 'fullyconnected0_output',\n",
       " 'activation2_output',\n",
       " 'fullyconnected1_weight',\n",
       " 'fullyconnected1_bias',\n",
       " 'fullyconnected1_output',\n",
       " 'softmax_label',\n",
       " 'softmax_output']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:08:34.991829. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 列出模型中的所有层\n",
    "internals = model.symbol.get_internals()\n",
    "internals.list_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:10: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> results' number 14489\n",
      ">> test_lbl' number 14489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:11:51.503186. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 确定新层\n",
    "fea_symbol = internals['activation2_output']\n",
    "new_model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=fea_symbol,\n",
    "                             numpy_batch_size=1,\n",
    "                             arg_params=model.arg_params,\n",
    "                             aux_params=model.aux_params,\n",
    "                             allow_extra_params=True)\n",
    "\n",
    "# 提取结果到arff文件\n",
    "test_set = train_set      # train_set[split_tt:], train_set\n",
    "test_lbl = label_set      # label_set[split_tt:], label_set\n",
    "\n",
    "results = []\n",
    "for i in range(len(test_set)):\n",
    "    test_val = np.array(test_set[i]).reshape(1, 1, 100, 100)\n",
    "    prob = new_model.predict(test_val)\n",
    "#     print '数据个数 * 特征维数：', len(test_set), '*', len(prob[0])\n",
    "#     print prob[0]\n",
    "#     sys.exit(0)\n",
    "    results.append(prob[0])\n",
    "\n",
    "print \">> results' number\", len(results)\n",
    "print \">> test_lbl' number\", len(test_lbl)\n",
    "arff_results = open('lenet.arff', 'w')\n",
    "arff_results.write('@relation lenet\\n')\n",
    "for i in range(len(results[0])):\n",
    "    arff_results.write('@attribute Feature' + str(i+1) + ' real\\n')\n",
    "# for i in range(len(test_lbl[0])):\n",
    "#     arff_results.write('@attribute class' + str(i+1) + ' {0.0,1.0}\\n')\n",
    "arff_results.write('@attribute class {0.0,1.0}\\n')\n",
    "arff_results.write('@data\\n')\n",
    "for i in range(len(results)):\n",
    "    # line =  ','.join(str(j) for j in results[i]) + ',' + ','.join(str(j) for j in test_lbl[i])\n",
    "    line =  ','.join(str(j) for j in results[i]) + ',' + str(test_lbl[i])\n",
    "    arff_results.write(line + '\\n')\n",
    "arff_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 正例数（标签1.0）： 13272\n",
      ">> 反例数（标签0.0）： 1218\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:16:20.660983. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "#  随机分割不平衡数据集，生成arff文件\n",
    "import random\n",
    "\n",
    "def split_unbalance(file_name, pos_lab, neg_lab):\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    suffix = ''\n",
    "    for line in open(file_name):\n",
    "        if line[0] == '@':\n",
    "            suffix += line\n",
    "        else:\n",
    "            if line.strip().split(',')[-1] == pos_lab:\n",
    "                pos_list.append(line.strip())\n",
    "            elif line.strip().split(',')[-1] == neg_lab:\n",
    "                neg_list.append(line.strip())\n",
    "    pos_len = len(pos_list)\n",
    "    neg_len = len(neg_list)\n",
    "    print '>> 正例数（标签1.0）：', pos_len\n",
    "    print '>> 反例数（标签0.0）：', neg_len\n",
    "    if pos_len > neg_len:\n",
    "        new_list = random.sample(pos_list, neg_len)\n",
    "        return suffix, new_list, neg_list\n",
    "    elif pos_len < neg_len:\n",
    "        new_list = random.sample(neg_list, pos_len)\n",
    "        return suffix, pos_list, new_list\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "input_file = \"lenet.arff\"\n",
    "output_file = \"lenet-b.arff\"\n",
    "pos_lab = \"1.0\"\n",
    "neg_lab = \"0.0\"\n",
    "suffix, one_list, two_list = split_unbalance(input_file, pos_lab, neg_lab)\n",
    "\n",
    "if None is suffix:\n",
    "    print 'Blanced Dataset !'\n",
    "    exit()\n",
    "\n",
    "one_list.extend(two_list)\n",
    "random.shuffle(one_list)\n",
    "results = open(output_file, \"w\")\n",
    "results.write(suffix)\n",
    "for i in range(len(one_list)):\n",
    "    results.write(one_list[i] + '\\n')\n",
    "results.close()\n",
    "\n",
    "print 'Finished.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* 调用sklearn做机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Time cost on loading  lenet-b.libsvm :  0.017421\n",
      "Dimension: 500\n",
      "* Classifier: LibSVM\n",
      "* Matrix:\n",
      "[[394   0]\n",
      " [410   0]]\n",
      "* Accuracy: 0.4900\n",
      "* F1 Score: 0.0000\n",
      "* ROC AUC Score: 0.5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:48:27.976794. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# encoding:utf-8\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import threading\n",
    "import math\n",
    "import numpy as np\n",
    "from time import clock\n",
    "\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn import cross_validation, metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n",
    "    BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(file_name):\n",
    "    data = load_svmlight_file(file_name)\n",
    "    return data[0], data[1]\n",
    "\n",
    "def arff2svm(arff_file):\n",
    "    name = arff_file[0: arff_file.rindex('.')]\n",
    "    tpe = arff_file[arff_file.rindex('.')+1:]\n",
    "    svm_file = name+\".libsvm\"\n",
    "    if tpe == \"arff\":\n",
    "        if os.path.exists(svm_file):\n",
    "            pass\n",
    "        else:\n",
    "            w = open(svm_file, 'w')\n",
    "            flag = False\n",
    "            for line in open(arff_file):\n",
    "                if flag:\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "                    temp = line.strip('\\n').split(',')\n",
    "                    w.write(temp[len(temp)-1])\n",
    "                    for i in range(len(temp)-1):\n",
    "                        w.write(' '+str(i+1)+':'+str(temp[i]))\n",
    "                    w.write('\\n')\n",
    "                else:\n",
    "                    line = line.upper()\n",
    "                    if line.startswith('@DATA') or flag:\n",
    "                        flag = True\n",
    "            w.close()\n",
    "    elif tpe == \"libsvm\":\n",
    "        return arff_file\n",
    "    else:\n",
    "        print \"File format error! Arff and libsvm are passed.\"\n",
    "        sys.exit()\n",
    "    return svm_file\n",
    "\n",
    "def loop_classifier(name, clf, train_x, train_y, test_x=None, test_y=None, cv=None):\n",
    "    try:\n",
    "        clf.fit(train_x, train_y)\n",
    "        # print name, \"Thread: \", 'Best Param: ', clf.best_params_\n",
    "        if cv is not None:\n",
    "            forecast = cross_validation.cross_val_predict(clf, train_x, train_y, cv=cv)\n",
    "            test_y = train_y\n",
    "        else:\n",
    "            forecast = clf.predict(test_x)\n",
    "        mat = metrics.confusion_matrix(test_y, forecast)\n",
    "        ac = '%0.4f' % metrics.accuracy_score(test_y, forecast)\n",
    "        fc = '%0.4f' % metrics.f1_score(test_y, forecast)\n",
    "        roc_auc_score = '%0.4f' % cross_validation.cross_val_score(\n",
    "            clf, train_x, train_y, scoring='roc_auc',cv=cv).mean()\n",
    "        print '* Classifier:', name\n",
    "        print '* Matrix:\\n', mat\n",
    "        print '* Accuracy:', ac\n",
    "        print '* F1 Score:', fc\n",
    "        print '* ROC AUC Score:', roc_auc_score\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "input_file = arff2svm('lenet-b.arff')\n",
    "sec = clock()\n",
    "cv = 5\n",
    "split_rate = 0.33\n",
    "X, y = get_data(input_file)\n",
    "X = X.todense()\n",
    "results = []\n",
    "print '*** Time cost on loading ', input_file, ': ', clock() - sec\n",
    "\n",
    "# 对数据切分或交叉验证，得出结果\n",
    "dimension = int(X.shape[1])\n",
    "print \"Dimension:\", dimension\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, \n",
    "                                                                     y, \n",
    "                                                                     test_size=split_rate, \n",
    "                                                                     random_state=0)\n",
    "classifier2 = Pipeline([('pca', PCA()), ('LibSVM', SVC())])\n",
    "grid_search = GridSearchCV(classifier2, param_grid=dict())\n",
    "loop_classifier('LibSVM', grid_search, X_train, y_train, test_x=X_test, test_y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "WEKA运行该平衡数据集的结果：\n",
    "\n",
    "![](img/weka-01.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
