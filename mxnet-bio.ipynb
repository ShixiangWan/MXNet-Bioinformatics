{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "DATE: 2017, 3, 17\n",
    "1. 说明：用one-hot编码，利用MXNet设计卷积神经网络（MLP，LeNET，CIFAR 10 NET），对RNA序列实现二分类、多分类，评价指标为mse，准确率，F1。\n",
    "\n",
    "2. Tricks：\n",
    " * 解决序列长短不一问题。程序先从所有样本中选出最长的序列，以它的长度depth作为共同特征矩阵的长和宽，也就是形成${\\rm{depth}} \\times {\\rm{depth}}$的矩阵，长度不够的位置补全零数组，类似于图片的稀疏像素。若depth为4，序列AUC可编码为：\n",
    "$$ \\left[ {\\begin{array}{*{20}{c}}\n",
    "0&1&0&0\\\\\n",
    "0&0&1&0\\\\\n",
    "0&0&0&1\\\\\n",
    "0&0&0&0\n",
    "\\end{array}} \\right] $$\n",
    "\n",
    " * 但是对于过长的序列会产生过高维度的特征，可以采用香农编码或哈夫曼编码压缩编码解决。\n",
    "\n",
    "DATE: 2017, 3, 24\n",
    "1. 用哈夫曼编码暂时不考虑，使用最简单的A=1，,U=2，C=3，G=4来编码，每条样本形成${\\rm{100}} \\times {\\rm{100}}$的矩阵，该种效果与直接使用pse-in-one提取出的特征分类相似；\n",
    "2. 考虑使用4个字母的连续256种组合编码，与图像的3通道像素相似。暂时无实验价值；\n",
    "3. 考虑直接将卷积出的抽象特征输出，连接SVM做分类。\n",
    "\n",
    "DATE: 2017, 3, 29\n",
    "1. 直接将卷积出的抽象特征输出，连接sklearn或weka做分类，效果都还是不好，为什么分类准确度这么低？50%？\n",
    "2. 有没有其他可行的方案？思考中......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 19:52:29.815224. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# coding=utf-8\n",
    "# 设计卷积网络\n",
    "import time\n",
    "\n",
    "import mxnet as mx\n",
    "import numpy as np\n",
    "\n",
    "import logging\n",
    "    \n",
    "# Basic Conv + BN + ReLU factory\n",
    "def ConvFactory(data, num_filter, kernel, stride=(1, 1), pad=(0, 0), act_type=\"relu\"):\n",
    "    # there is an optional parameter ```wrokshpace``` may influece convolution performance\n",
    "    # default, the workspace is set to 256(MB)\n",
    "    # you may set larger value, but convolution layer only requires its needed but not exactly\n",
    "    # MXNet will handle reuse of workspace without parallelism conflict\n",
    "    conv = mx.symbol.Convolution(data=data, workspace=256,\n",
    "                                 num_filter=num_filter, kernel=kernel, stride=stride, pad=pad)\n",
    "    bn = mx.symbol.BatchNorm(data=conv)\n",
    "    act = mx.symbol.Activation(data=bn, act_type=act_type)\n",
    "    return act\n",
    "\n",
    "\n",
    "# A Simple Downsampling Factory\n",
    "def DownsampleFactory(data, ch_3x3):\n",
    "    # conv 3x3\n",
    "    conv = ConvFactory(data=data, kernel=(3, 3), stride=(2, 2), num_filter=ch_3x3, pad=(1, 1))\n",
    "    # pool\n",
    "    pool = mx.symbol.Pooling(data=data, kernel=(3, 3), stride=(2, 2), pad=(1, 1), pool_type='max')\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv, pool])\n",
    "    return concat\n",
    "\n",
    "\n",
    "# A Simple module\n",
    "def SimpleFactory(data, ch_1x1, ch_3x3):\n",
    "    # 1x1\n",
    "    conv1x1 = ConvFactory(data=data, kernel=(1, 1), pad=(0, 0), num_filter=ch_1x1)\n",
    "    # 3x3\n",
    "    conv3x3 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=ch_3x3)\n",
    "    # concat\n",
    "    concat = mx.symbol.Concat(*[conv1x1, conv3x3])\n",
    "    return concat\n",
    "\n",
    "\n",
    "def set_cifar10():\n",
    "    data = mx.symbol.Variable(name=\"data\")\n",
    "    conv1 = ConvFactory(data=data, kernel=(3, 3), pad=(1, 1), num_filter=96, act_type=\"relu\")\n",
    "    in3a = SimpleFactory(conv1, 32, 32)\n",
    "    in3b = SimpleFactory(in3a, 32, 48)\n",
    "    in3c = DownsampleFactory(in3b, 80)\n",
    "    in4a = SimpleFactory(in3c, 112, 48)\n",
    "    in4b = SimpleFactory(in4a, 96, 64)\n",
    "    in4c = SimpleFactory(in4b, 80, 80)\n",
    "    in4d = SimpleFactory(in4c, 48, 96)\n",
    "    in4e = DownsampleFactory(in4d, 96)\n",
    "    in5a = SimpleFactory(in4e, 176, 160)\n",
    "    in5b = SimpleFactory(in5a, 176, 160)\n",
    "    pool = mx.symbol.Pooling(data=in5b, pool_type=\"avg\", kernel=(7, 7), name=\"global_avg\")\n",
    "    flatten = mx.symbol.Flatten(data=pool)\n",
    "    fc = mx.symbol.FullyConnected(data=flatten, num_hidden=10)\n",
    "    softmax = mx.symbol.SoftmaxOutput(name='softmax', data=fc)\n",
    "    return softmax\n",
    "\n",
    "\n",
    "# 卷积神经网络\n",
    "def set_con():\n",
    "    data = mx.symbol.Variable('data')\n",
    "    # first conv layer\n",
    "    mx.sym.Activation\n",
    "    conv1 = mx.sym.Convolution(data=data, kernel=(5, 5), num_filter=20)\n",
    "    tanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\n",
    "    pool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # second conv layer\n",
    "    conv2 = mx.sym.Convolution(data=pool1, kernel=(5, 5), num_filter=50)\n",
    "    tanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\n",
    "    pool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2, 2), stride=(2, 2))\n",
    "    # first fullc layer\n",
    "    flatten = mx.sym.Flatten(data=pool2)\n",
    "    fc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\n",
    "    tanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n",
    "    # second fullc\n",
    "    fc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n",
    "    # softmax loss\n",
    "    lenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax') \n",
    "    return lenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> loading datasets ... \n",
      "max depth: 55676\n",
      "time cost: 3.540327 s\n",
      ">> encoding ...  finished. 36.323117 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 19:55:46.496857. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 载入数据，并进行one-hot编码\n",
    "\n",
    "import random\n",
    "\n",
    "def load_data(encode='common'):\n",
    "    data_list = []\n",
    "    start = time.clock()\n",
    "    print '>> loading datasets ... '\n",
    "    for line in open('/home01/shixiangwan/deep_learning/protein_location/slhp-cd0.7.fasta'):\n",
    "        if line[0] != '>':\n",
    "            line = line.strip()\n",
    "            # DNA/RNA\n",
    "            # line = replace('A', '1').replace('U', '2').replace('C', '3').replace('G', '4')\n",
    "            # Protein\n",
    "            line = line.replace('A', '1')\n",
    "            line = line.replace('B', '2')\n",
    "            line = line.replace('C', '3')\n",
    "            line = line.replace('D', '4')\n",
    "            line = line.replace('E', '5')\n",
    "            line = line.replace('F', '6')\n",
    "            line = line.replace('G', '7')\n",
    "            line = line.replace('H', '8')\n",
    "            line = line.replace('I', '9')\n",
    "            line = line.replace('J', '10')\n",
    "            line = line.replace('K', '11')\n",
    "            line = line.replace('L', '12')\n",
    "            line = line.replace('M', '13')\n",
    "            line = line.replace('N', '14')\n",
    "            line = line.replace('O', '15')\n",
    "            line = line.replace('P', '16')\n",
    "            line = line.replace('Q', '17')\n",
    "            line = line.replace('R', '18')\n",
    "            line = line.replace('S', '19')\n",
    "            line = line.replace('T', '20')\n",
    "            line = line.replace('U', '21')\n",
    "            line = line.replace('V', '22')\n",
    "            line = line.replace('W', '23')\n",
    "            line = line.replace('X', '24')\n",
    "            line = line.replace('Y', '25')\n",
    "            line = line.replace('Z', '26')\n",
    "            data_list.append(map(int, list(line)))\n",
    "    depth = len(max(data_list, key=len))\n",
    "    print 'max depth:', depth\n",
    "    print 'time cost:', time.clock() - start, 's'\n",
    "\n",
    "    print '>> encoding ... '\n",
    "    train_set = []\n",
    "    if encode == 'onehot':\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_mx = mx.nd.one_hot(mx.nd.array(data_list[elem], dtype=np.int32), depth=depth) \\\n",
    "                .asnumpy().reshape(len(data_list[elem]) * depth)\n",
    "            tmp_mx = np.append(tmp_mx, np.zeros(len(data_list[elem]) * depth))\n",
    "            train_set.append(list(tmp_mx))\n",
    "    else:\n",
    "        for elem in range(len(data_list)):\n",
    "            tmp_list = data_list[elem]\n",
    "            tmp_mx = np.append(tmp_list, np.zeros(60000 - len(tmp_list)))\n",
    "            train_set.append(list(tmp_mx))\n",
    "\n",
    "    label_set = []\n",
    "    for line in open(\"slhp_labels_10.txt\"):\n",
    "        label_set.append(map(float, line.strip().split(',')))   # multi-label\n",
    "        # label_set.append(float(line.strip()))                   # single-label\n",
    "    print 'time cost:', time.clock() - start, 's'\n",
    "    random.seed(100)\n",
    "    random.shuffle(train_set)\n",
    "    random.seed(100)\n",
    "    random.shuffle(label_set)\n",
    "    return train_set, label_set\n",
    "\n",
    "train_set, label_set = load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 19:57:06.641148. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 将数据变成类似图像的矩阵形式\n",
    "\n",
    "def to4d(data):\n",
    "    print 'data.shape:', data.shape\n",
    "    return data.reshape(data.shape[0], 1, 300, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data.shape: (7886, 60000)\n",
      "data.shape: (986, 60000)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:22: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n",
      "INFO:root:Start training with [gpu(1)]\n",
      "INFO:root:Epoch[0] Batch [200]\tSpeed: 2979.78 samples/sec\tTrain-accuracy=0.830000\n",
      "INFO:root:Epoch[0] Batch [400]\tSpeed: 2983.38 samples/sec\tTrain-accuracy=0.817500\n",
      "INFO:root:Epoch[0] Batch [600]\tSpeed: 2985.30 samples/sec\tTrain-accuracy=0.816000\n",
      "INFO:root:Epoch[0] Batch [800]\tSpeed: 2983.42 samples/sec\tTrain-accuracy=0.820500\n",
      "INFO:root:Epoch[0] Batch [1000]\tSpeed: 2980.35 samples/sec\tTrain-accuracy=0.821000\n",
      "INFO:root:Epoch[0] Batch [1200]\tSpeed: 2984.12 samples/sec\tTrain-accuracy=0.811500\n",
      "INFO:root:Epoch[0] Batch [1400]\tSpeed: 2980.56 samples/sec\tTrain-accuracy=0.826500\n",
      "INFO:root:Epoch[0] Batch [1600]\tSpeed: 2980.96 samples/sec\tTrain-accuracy=0.820500\n",
      "INFO:root:Epoch[0] Batch [1800]\tSpeed: 2983.68 samples/sec\tTrain-accuracy=0.828000\n",
      "INFO:root:Epoch[0] Batch [2000]\tSpeed: 2974.88 samples/sec\tTrain-accuracy=0.819500\n",
      "INFO:root:Epoch[0] Batch [2200]\tSpeed: 2979.64 samples/sec\tTrain-accuracy=0.824000\n",
      "INFO:root:Epoch[0] Batch [2400]\tSpeed: 2977.24 samples/sec\tTrain-accuracy=0.837000\n",
      "INFO:root:Epoch[0] Batch [2600]\tSpeed: 2976.32 samples/sec\tTrain-accuracy=0.821000\n",
      "INFO:root:Epoch[0] Batch [2800]\tSpeed: 2977.33 samples/sec\tTrain-accuracy=0.817500\n",
      "INFO:root:Epoch[0] Batch [3000]\tSpeed: 2977.56 samples/sec\tTrain-accuracy=0.823500\n",
      "INFO:root:Epoch[0] Batch [3200]\tSpeed: 2981.00 samples/sec\tTrain-accuracy=0.825500\n",
      "INFO:root:Epoch[0] Batch [3400]\tSpeed: 2980.53 samples/sec\tTrain-accuracy=0.820000\n",
      "INFO:root:Epoch[0] Batch [3600]\tSpeed: 2979.43 samples/sec\tTrain-accuracy=0.826000\n",
      "INFO:root:Epoch[0] Batch [3800]\tSpeed: 2980.97 samples/sec\tTrain-accuracy=0.802000\n",
      "INFO:root:Epoch[0] Batch [4000]\tSpeed: 2978.56 samples/sec\tTrain-accuracy=0.828000\n",
      "INFO:root:Epoch[0] Batch [4200]\tSpeed: 2980.96 samples/sec\tTrain-accuracy=0.819500\n",
      "INFO:root:Epoch[0] Batch [4400]\tSpeed: 2984.52 samples/sec\tTrain-accuracy=0.824500\n",
      "INFO:root:Epoch[0] Batch [4600]\tSpeed: 2978.52 samples/sec\tTrain-accuracy=0.826000\n",
      "INFO:root:Epoch[0] Batch [4800]\tSpeed: 2981.62 samples/sec\tTrain-accuracy=0.815500\n",
      "INFO:root:Epoch[0] Batch [5000]\tSpeed: 2981.64 samples/sec\tTrain-accuracy=0.822500\n",
      "INFO:root:Epoch[0] Batch [5200]\tSpeed: 2978.73 samples/sec\tTrain-accuracy=0.817000\n",
      "INFO:root:Epoch[0] Batch [5400]\tSpeed: 2980.45 samples/sec\tTrain-accuracy=0.818500\n",
      "INFO:root:Epoch[0] Batch [5600]\tSpeed: 2978.27 samples/sec\tTrain-accuracy=0.816500\n",
      "INFO:root:Epoch[0] Batch [5800]\tSpeed: 2977.95 samples/sec\tTrain-accuracy=0.826000\n",
      "INFO:root:Epoch[0] Batch [6000]\tSpeed: 2979.20 samples/sec\tTrain-accuracy=0.835000\n",
      "INFO:root:Epoch[0] Batch [6200]\tSpeed: 2980.95 samples/sec\tTrain-accuracy=0.814500\n",
      "INFO:root:Epoch[0] Batch [6400]\tSpeed: 2983.82 samples/sec\tTrain-accuracy=0.820500\n",
      "INFO:root:Epoch[0] Batch [6600]\tSpeed: 2976.95 samples/sec\tTrain-accuracy=0.812000\n",
      "INFO:root:Epoch[0] Batch [6800]\tSpeed: 2983.23 samples/sec\tTrain-accuracy=0.822000\n",
      "INFO:root:Epoch[0] Batch [7000]\tSpeed: 2981.75 samples/sec\tTrain-accuracy=0.808500\n",
      "INFO:root:Epoch[0] Batch [7200]\tSpeed: 2984.59 samples/sec\tTrain-accuracy=0.829500\n",
      "INFO:root:Epoch[0] Batch [7400]\tSpeed: 2981.65 samples/sec\tTrain-accuracy=0.813000\n",
      "INFO:root:Epoch[0] Batch [7600]\tSpeed: 2979.56 samples/sec\tTrain-accuracy=0.823000\n",
      "INFO:root:Epoch[0] Batch [7800]\tSpeed: 2980.07 samples/sec\tTrain-accuracy=0.813500\n",
      "INFO:root:Epoch[0] Resetting Data Iterator\n",
      "INFO:root:Epoch[0] Time cost=265.337\n",
      "INFO:root:Epoch[0] Validation-accuracy=0.830832\n",
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 19:59:46.655133. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 对已经载入的数据进行深度学习\n",
    "\n",
    "all_number = len(label_set)\n",
    "# print all_number\n",
    "# print np.array(train_set).shape\n",
    "split_tv = int(0.8 * all_number)\n",
    "split_tt = int(0.9 * all_number)\n",
    "train_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[0:split_tv]))),\n",
    "                               mx.nd.array(np.array(label_set[0:split_tv])),\n",
    "                               shuffle=True)\n",
    "validate_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(train_set[split_tv:split_tt]))),\n",
    "                                  mx.nd.array(np.array(label_set[split_tv:split_tt])),\n",
    "                                  shuffle=True)\n",
    "\n",
    "# train\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=set_con(),  # set_mlp(), set_con(), set_cifar10()\n",
    "                             num_epoch=1,\n",
    "                             learning_rate=0.1,\n",
    "                             momentum=0.9,\n",
    "                             wd=0.00001)\n",
    "\n",
    "batch_size = 100\n",
    "result = model.fit(X=train_iter,\n",
    "                   eval_data=validate_iter,\n",
    "                   batch_end_callback=mx.callback.Speedometer(batch_size, 200)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "1.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "[  1.56474986e-10   1.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "   0.00000000e+00   0.00000000e+00]\n",
      "1.56475e-10,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0\n",
      "0.0,1.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0\n",
      "\n",
      "data.shape: (986, 60000)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-d94a0c8421f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                               \u001b[0mmx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_lbl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                               shuffle=True)\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0mf1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'----------------------------------------------------------------------'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m'precision\\t\\t'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'mse'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.9.4-py2.7.egg/mxnet/model.pyc\u001b[0m in \u001b[0;36mscore\u001b[0;34m(self, X, eval_metric, num_batch, batch_end_callback, reset)\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_arrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pred_exec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m             \u001b[0meval_metric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pred_exec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_end_callback\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.9.4-py2.7.egg/mxnet/metric.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, labels, preds)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0mmetric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/mxnet-0.9.4-py2.7.egg/mxnet/metric.pyc\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, labels, preds)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_label\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m                     \u001b[0mtrue_positives\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-04-03 20:10:37.090950. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 分析结果，直接将每个标签的预测结果、原结果存储起来\n",
    "\n",
    "test_set = train_set[split_tt:]\n",
    "test_lbl = label_set[split_tt:]\n",
    "\n",
    "count = 0\n",
    "r_num = 0\n",
    "results = open('results.csv', 'w')\n",
    "for i in range(len(test_set)):\n",
    "    test_val = np.array(test_set[i]).reshape(1, 1, 300, 200)\n",
    "    prob = model.predict(test_val)\n",
    "    print prob[0]\n",
    "    predict =  ','.join(str(j) for j in prob[0])\n",
    "    origin = ','.join(str(j) for j in test_lbl[i])\n",
    "    # origin = str(test_lbl[i])\n",
    "    for j in range(len(prob[0])):\n",
    "        p_val = 0\n",
    "        if prob[0][j] >= 0.5:\n",
    "            p_val = 1\n",
    "        if p_val == test_lbl[i][j]:\n",
    "        # if p_val == test_lbl[i]:\n",
    "            r_num += 1\n",
    "    results.write(predict + '\\n' + origin + '\\n\\n')\n",
    "    count += 1\n",
    "#     print predict\n",
    "#     print origin\n",
    "#     print\n",
    "#     if count == 10:\n",
    "#         break\n",
    "results.close()\n",
    "\n",
    "# performance matrics\n",
    "test_iter = mx.io.NDArrayIter(mx.nd.array(to4d(np.array(test_set))),\n",
    "                              mx.nd.array(np.array(test_lbl)),\n",
    "                              shuffle=True)\n",
    "f1, mse, acc = model.score(test_iter, ['f1', 'mse', 'acc'])\n",
    "print '----------------------------------------------------------------------'\n",
    "print 'precision\\t\\t' + 'mse'\n",
    "print acc, '\\t\\t', mse, '\\t\\t', f1\n",
    "\n",
    "# print '\\nright/all/rate:', r_num, count*len(label_set[0]), float(r_num)/(count*len(label_set[0]))\n",
    "print '\\nright/all/rate:', r_num, count, float(r_num)/(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'convolution0_weight',\n",
       " 'convolution0_bias',\n",
       " 'convolution0_output',\n",
       " 'activation0_output',\n",
       " 'pooling0_output',\n",
       " 'convolution1_weight',\n",
       " 'convolution1_bias',\n",
       " 'convolution1_output',\n",
       " 'activation1_output',\n",
       " 'pooling1_output',\n",
       " 'flatten0_output',\n",
       " 'fullyconnected0_weight',\n",
       " 'fullyconnected0_bias',\n",
       " 'fullyconnected0_output',\n",
       " 'activation2_output',\n",
       " 'fullyconnected1_weight',\n",
       " 'fullyconnected1_bias',\n",
       " 'fullyconnected1_output',\n",
       " 'softmax_label',\n",
       " 'softmax_output']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:08:34.991829. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# 列出模型中的所有层\n",
    "internals = model.symbol.get_internals()\n",
    "internals.list_outputs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shixiangwan/.local/lib/python2.7/site-packages/ipykernel/__main__.py:10: DeprecationWarning: \u001b[91mmxnet.model.FeedForward has been deprecated. Please use mxnet.mod.Module instead.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> results' number 14489\n",
      ">> test_lbl' number 14489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:11:51.503186. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "# 确定新层\n",
    "fea_symbol = internals['activation2_output']\n",
    "new_model = mx.model.FeedForward(ctx=mx.gpu(1),  # [mx.gpu(i) for i in range(4)]\n",
    "                             symbol=fea_symbol,\n",
    "                             numpy_batch_size=1,\n",
    "                             arg_params=model.arg_params,\n",
    "                             aux_params=model.aux_params,\n",
    "                             allow_extra_params=True)\n",
    "\n",
    "# 提取结果到arff文件\n",
    "test_set = train_set      # train_set[split_tt:], train_set\n",
    "test_lbl = label_set      # label_set[split_tt:], label_set\n",
    "\n",
    "results = []\n",
    "for i in range(len(test_set)):\n",
    "    test_val = np.array(test_set[i]).reshape(1, 1, 100, 100)\n",
    "    prob = new_model.predict(test_val)\n",
    "#     print '数据个数 * 特征维数：', len(test_set), '*', len(prob[0])\n",
    "#     print prob[0]\n",
    "#     sys.exit(0)\n",
    "    results.append(prob[0])\n",
    "\n",
    "print \">> results' number\", len(results)\n",
    "print \">> test_lbl' number\", len(test_lbl)\n",
    "arff_results = open('lenet.arff', 'w')\n",
    "arff_results.write('@relation lenet\\n')\n",
    "for i in range(len(results[0])):\n",
    "    arff_results.write('@attribute Feature' + str(i+1) + ' real\\n')\n",
    "# for i in range(len(test_lbl[0])):\n",
    "#     arff_results.write('@attribute class' + str(i+1) + ' {0.0,1.0}\\n')\n",
    "arff_results.write('@attribute class {0.0,1.0}\\n')\n",
    "arff_results.write('@data\\n')\n",
    "for i in range(len(results)):\n",
    "    # line =  ','.join(str(j) for j in results[i]) + ',' + ','.join(str(j) for j in test_lbl[i])\n",
    "    line =  ','.join(str(j) for j in results[i]) + ',' + str(test_lbl[i])\n",
    "    arff_results.write(line + '\\n')\n",
    "arff_results.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> 正例数（标签1.0）： 13272\n",
      ">> 反例数（标签0.0）： 1218\n",
      "Finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:16:20.660983. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "#  随机分割不平衡数据集，生成arff文件\n",
    "import random\n",
    "\n",
    "def split_unbalance(file_name, pos_lab, neg_lab):\n",
    "    pos_list = []\n",
    "    neg_list = []\n",
    "    suffix = ''\n",
    "    for line in open(file_name):\n",
    "        if line[0] == '@':\n",
    "            suffix += line\n",
    "        else:\n",
    "            if line.strip().split(',')[-1] == pos_lab:\n",
    "                pos_list.append(line.strip())\n",
    "            elif line.strip().split(',')[-1] == neg_lab:\n",
    "                neg_list.append(line.strip())\n",
    "    pos_len = len(pos_list)\n",
    "    neg_len = len(neg_list)\n",
    "    print '>> 正例数（标签1.0）：', pos_len\n",
    "    print '>> 反例数（标签0.0）：', neg_len\n",
    "    if pos_len > neg_len:\n",
    "        new_list = random.sample(pos_list, neg_len)\n",
    "        return suffix, new_list, neg_list\n",
    "    elif pos_len < neg_len:\n",
    "        new_list = random.sample(neg_list, pos_len)\n",
    "        return suffix, pos_list, new_list\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "input_file = \"lenet.arff\"\n",
    "output_file = \"lenet-b.arff\"\n",
    "pos_lab = \"1.0\"\n",
    "neg_lab = \"0.0\"\n",
    "suffix, one_list, two_list = split_unbalance(input_file, pos_lab, neg_lab)\n",
    "\n",
    "if None is suffix:\n",
    "    print 'Blanced Dataset !'\n",
    "    exit()\n",
    "\n",
    "one_list.extend(two_list)\n",
    "random.shuffle(one_list)\n",
    "results = open(output_file, \"w\")\n",
    "results.write(suffix)\n",
    "for i in range(len(one_list)):\n",
    "    results.write(one_list[i] + '\\n')\n",
    "results.close()\n",
    "\n",
    "print 'Finished.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* 调用sklearn做机器学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Time cost on loading  lenet-b.libsvm :  0.017421\n",
      "Dimension: 500\n",
      "* Classifier: LibSVM\n",
      "* Matrix:\n",
      "[[394   0]\n",
      " [410   0]]\n",
      "* Accuracy: 0.4900\n",
      "* F1 Score: 0.0000\n",
      "* ROC AUC Score: 0.5000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/json/encoder.py:207: DeprecationWarning: Interpreting naive datetime as local 2017-03-29 15:48:27.976794. Please add timezone info to timestamps.\n",
      "  chunks = self.iterencode(o, _one_shot=True)\n"
     ]
    }
   ],
   "source": [
    "# encoding:utf-8\n",
    "import os\n",
    "import sys\n",
    "import getopt\n",
    "import threading\n",
    "import math\n",
    "import numpy as np\n",
    "from time import clock\n",
    "\n",
    "from sklearn.externals.joblib import Memory\n",
    "from sklearn import cross_validation, metrics\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, \\\n",
    "    BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "\n",
    "mem = Memory(\"./mycache\")\n",
    "@mem.cache\n",
    "def get_data(file_name):\n",
    "    data = load_svmlight_file(file_name)\n",
    "    return data[0], data[1]\n",
    "\n",
    "def arff2svm(arff_file):\n",
    "    name = arff_file[0: arff_file.rindex('.')]\n",
    "    tpe = arff_file[arff_file.rindex('.')+1:]\n",
    "    svm_file = name+\".libsvm\"\n",
    "    if tpe == \"arff\":\n",
    "        if os.path.exists(svm_file):\n",
    "            pass\n",
    "        else:\n",
    "            w = open(svm_file, 'w')\n",
    "            flag = False\n",
    "            for line in open(arff_file):\n",
    "                if flag:\n",
    "                    if line.strip() == '':\n",
    "                        continue\n",
    "                    temp = line.strip('\\n').split(',')\n",
    "                    w.write(temp[len(temp)-1])\n",
    "                    for i in range(len(temp)-1):\n",
    "                        w.write(' '+str(i+1)+':'+str(temp[i]))\n",
    "                    w.write('\\n')\n",
    "                else:\n",
    "                    line = line.upper()\n",
    "                    if line.startswith('@DATA') or flag:\n",
    "                        flag = True\n",
    "            w.close()\n",
    "    elif tpe == \"libsvm\":\n",
    "        return arff_file\n",
    "    else:\n",
    "        print \"File format error! Arff and libsvm are passed.\"\n",
    "        sys.exit()\n",
    "    return svm_file\n",
    "\n",
    "def loop_classifier(name, clf, train_x, train_y, test_x=None, test_y=None, cv=None):\n",
    "    try:\n",
    "        clf.fit(train_x, train_y)\n",
    "        # print name, \"Thread: \", 'Best Param: ', clf.best_params_\n",
    "        if cv is not None:\n",
    "            forecast = cross_validation.cross_val_predict(clf, train_x, train_y, cv=cv)\n",
    "            test_y = train_y\n",
    "        else:\n",
    "            forecast = clf.predict(test_x)\n",
    "        mat = metrics.confusion_matrix(test_y, forecast)\n",
    "        ac = '%0.4f' % metrics.accuracy_score(test_y, forecast)\n",
    "        fc = '%0.4f' % metrics.f1_score(test_y, forecast)\n",
    "        roc_auc_score = '%0.4f' % cross_validation.cross_val_score(\n",
    "            clf, train_x, train_y, scoring='roc_auc',cv=cv).mean()\n",
    "        print '* Classifier:', name\n",
    "        print '* Matrix:\\n', mat\n",
    "        print '* Accuracy:', ac\n",
    "        print '* F1 Score:', fc\n",
    "        print '* ROC AUC Score:', roc_auc_score\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "input_file = arff2svm('lenet-b.arff')\n",
    "sec = clock()\n",
    "cv = 5\n",
    "split_rate = 0.33\n",
    "X, y = get_data(input_file)\n",
    "X = X.todense()\n",
    "results = []\n",
    "print '*** Time cost on loading ', input_file, ': ', clock() - sec\n",
    "\n",
    "# 对数据切分或交叉验证，得出结果\n",
    "dimension = int(X.shape[1])\n",
    "print \"Dimension:\", dimension\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(X, \n",
    "                                                                     y, \n",
    "                                                                     test_size=split_rate, \n",
    "                                                                     random_state=0)\n",
    "classifier2 = Pipeline([('pca', PCA()), ('LibSVM', SVC())])\n",
    "grid_search = GridSearchCV(classifier2, param_grid=dict())\n",
    "loop_classifier('LibSVM', grid_search, X_train, y_train, test_x=X_test, test_y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "WEKA运行该平衡数据集的结果：\n",
    "\n",
    "![](img/weka-01.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
